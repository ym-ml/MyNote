# 1.子集搜索与评价
属性--->特征(feature)
有的特征和我们要解决的问题关系很大-->相关特征
有的没什么用-->无关特征
so,要选择相关特征,即**特征选择** 

特征选择
	1.解决维度灾难(与降维动机接近)
	2.降低学习难度
	3.**注意**:有一些 **"冗余特征"**(redundant feature) -->可以由别的特征推导出来,
		比如已知长方体底面长和宽,底面积就是冗余特征.<br>
		通常冗余特征没用,但是,如果要预测的是体积,冗余特征可以降低学习难度,即此时成为了一种 **"中间概念"**
		eg:
			![[Pasted image 20251217202214.png]]
			现在有测试数据6和-1
			对第一组数据:![[Pasted image 20251217202500.png]]
			对第二组加了"冗余特征"的:![[Pasted image 20251217202640.png]]
			原训练数据接近y=(x-3)<sup>2</sup> ,显然第二组结果更准确<br>
			事实上,这属于**特征工程**中的多项式特征扩展,与核技巧也有异曲同工之妙
		[[特征工程 |特征工程--> 衍生特征]]
		本章假定数据中不涉及冗余特征 ^redundant

## 1.1环节
**核心**:从所有特征中筛选相关特征,怎么判断相关/无关,怎么去选出来

1)子集搜索(subset search)
	产生一个候选子集,然后添加/减少
	向前/向后/双向<br>
	向前:
		d个特征,a<sub>1</sub>,...a<sub>d</sub>,第一轮对每个特征,判断,挑出来一个最相关的,放入候选子集eg:{a<sub>3</sub>};
		然后对其余每个特征,和候选子集一起,再次判断,挑出最相关的eg:{a<sub>3</sub>,a<sub>5</sub>},然后和前一次的比较,如果比前一个效果好,加入候选子集,如果不如前一次,则退出;
		不断重复.
	向后:就是候选子集从全集开始,每次去掉一个
	双向:
		试图改进向前/向后每次只能考虑到局部的缺点,检查最后加入的k个特征.*(or走k步之后检查已有的候选子集?)*<br>
		比如向前,设置一个k,每次加入新特征之后,尝试依次删掉之前的k个特征,如果效果不变差,则确定删掉*(or每走k步,对已有候选子集做一个向后搜索)*<br>
		*貌似和书上意思不太一样*
2)子集评价(subset evaluation)
	1.基于信息增益
		给定数据集D,第i类样本占比为p<sub>i</sub>,根据候选子集A中属性的取值,将D分为了V个子集,每个子集中各属性取值相同.
			eg:
			`根蒂=硬挺&&色泽=青绿    根蒂=软&&色泽=青绿`
			`根蒂=硬挺&&色泽=乌黑    根蒂=软&&色泽=乌黑`
		则属性子集A的信息增益:
		![[Pasted image 20251217210859.png]]
		![[Pasted image 20251217210914.png]]
		信息增益越大,意味着特征子集A包含的有助于分类的信息越多
	2.许多"多样性度量"稍加调整即可用于特征子集评价(见集成学习笔记)
	
事实上,决策树就可以看作是一种特征选择

常见的特征选择方法有三类:**过滤式(filter)**,**包裹式(wrapper)**,**嵌入式(embedding)**

# 2.过滤式选择
>[!tldr]
>先对数据集进行特征选择("过滤"),然后再训练学习器

### Relief (对二分类):
设计一个"==相关统计量=="来度量特征的重要性(一个向量,每一个分量分别对应一个初始特征,**特征子集**的重要性由子集中所有特征对应的**分量之和**来确定)    *根据相关统计量来评价搜索到的子集*

### 关键:
**确定相关统计量.--->基于距离**
给定m个样本,对每一个样本x<sub>i</sub>,在同类样本中寻找最近邻,x<sub>i,nh</sub>(猜中近邻,near-hit);再从异类样本中寻找最近邻,x<sub>i,nm</sub>(猜错近邻,near-miss),然后统计相关量对应于属性j的分量为:
>![[Pasted image 20251217214229.png]]

在属性j上,x<sub>i</sub>和猜中近邻的区别更小(更容易猜对),则这个属性对分类更有益  
*已有度量学习的意味,这个diff也可以理解为"距离"*

这里可以只采样,而不是用所有的样本计算,因此运行效率很高

### 扩展:Relief-F(多分类)
找出所有异类的猜错近邻,做一个加权平均
>![[Pasted image 20251217215059.png]]

# 3.包裹式选择
>[!tldr]
>将学习器的性能作为特征子集的评价准则.
>
>最终性能比过滤式好,但是训练开销很大
### LVW(Las Vegas Wrapper)
在拉斯维加斯方法的框架下(*赌徒*)
使用**随机**策略产生特征子集A,在特征子集A上做**交叉验证**,如果泛化错误率低与之前或泛化错误率相等但是A中元素个数少,则保留A.重复直到达到限制条件或收敛

在有限时间内可能给不出结果
>![[Pasted image 20251217215941.png]]

还有一种蒙特卡罗方法


# 4.嵌入式选择与L1正则化
>[!tldr]
>过滤式和包裹式中特征选择和学习器训练有明显区别
>
>而嵌入式特征选择将**特征选择**与**学习器训练**两个过程融为一体


给定数据集D,考虑最简单的线性回归,以平方误差为损失函数.
>![[Pasted image 20251218104344.png]]

而样本特征过多,样本数过少时,容易过拟合,此时,常引入正则化项.如L2范数正则化
>![[Pasted image 20251218104511.png]]

也可以采用L1范数,此时相比L2,还有一个好处--更易于获得==稀疏(sparse)解==,即w中有更多分量为0(便可起到**特征选择**的作用)
	L1范数:所有分量的绝对值之和
	若想得到稀疏解,最自然的是L0范数,但是L0范数不连续,难以优化
>![[Pasted image 20251218105126.png]]
>*LASSO直译:最小绝对收缩选择算子*^LASSO

[[#^dictionary]]       
[[dictionary&LASSO]] ^b6d558

为了便于理解为何更易于稀疏解,以有两个特征为例,则(11.6)和(11.7)都会得到两个w,w<sub>1</sub>,w<sub>2</sub>,然后如图
>![[Pasted image 20251218105459.png]]

使用L1范数,w中的一些分量更趋近于0
## 求解:近端梯度下降(PGD)
>[!tldr]
>PGD适合求解目标函数由**可微**和**不可微**两部分组成的问题

优化目标:(这里的x就是要求的参数w)
>![[Pasted image 20251218111338.png]]

先对前半部分可微的做普通的梯度下降
*这里通过泰勒展开近似来求解,好处:可通过 Lipschitz 常数 L 确定最大安全步长*
*不用也行*
	L-Lipschitz:
	如果一个函数满足Lipschitz条件，则存在一个常数L，使得对于所有的x和y，函数的变化量不超过L乘以x和y之间的距离。
>![[Pasted image 20251218112934.png]]
>![[Pasted image 20251218112958.png]]
这里就获得了梯度下降的步长:1/L

而将(11.10)推广到(11.8),得到每一步迭代:
>![[Pasted image 20251218113154.png]]

求解(11.12):
>![[Pasted image 20251218113339.png]]
即先对前半部分做梯度下降,得到临时变量z

>![[Pasted image 20251218113552.png]]

**(11.13)具体求解:**
	 **1. 问题拆解**
		式 (11.13) 为:$$x_{k+1} = \arg \min_{x} \frac{L}{2} \| x - z \|_2^2 + \lambda \| x \|_1.$$
		目标函数写为分量形式（设 $x, z \in \mathbb{R}^n$）:$$\frac{L}{2} \sum_{i=1}^n (x^i - z^i)^2 + \lambda \sum_{i=1}^n |x^i|$$
		由于目标函数是各分量之和，且没有交叉项（即 \(x^i x^j\ (i \neq j)\)），因此可以**对每个分量独立求解最小值**。对于第 \(i\) 个分量，问题简化为：$$\min_{x^i} \left\{ \frac{L}{2} (x^i - z^i)^2 + \lambda |x^i| \right\}$$
	**2. 简化为一元优化问题**
		令 $u = x^i, a = z^i, 常数 c = \lambda / L$，则问题等价于：$$\min_{u \in \mathbb{R}} \left\{ \frac{1}{2} (u - a)^2 + c |u| \right\}$$其中乘数 L/2 不影响极值点（可约去公因子 L)<br>
		==定义函数==：$$h(u) = \frac{1}{2} (u - a)^2 + c |u|$$因为$|u| 在 u=0$ 处不可导，需分情况讨论。
	**3. 分情况求解**
		**情况 1：\(u > 0\)**
			此时 \(|u| = u\)，有：$$h(u) = \frac{1}{2} (u - a)^2 + c u$$求导并令导数为零:$$h'(u) = (u - a) + c = 0 \quad \Rightarrow \quad u = a - c$$该解成立需满足 \(u > 0\)，即 \(a - c > 0\)，亦即 \(a > c\)
		**情况 2：\(u < 0\)**
			此时 \(|u| = -u\)，有：$$h(u) = \frac{1}{2} (u - a)^2 - c u$$求导并令导数为零：$$h'(u) = (u - a) - c = 0 \quad \Rightarrow \quad u = a + c$$该解成立需满足 \(u < 0\)，即 \(a + c < 0\)，亦即 \(a < -c\)。
		 **情况 3：\(u = 0\)**
			 由于 \(|u|\) 在 \(u=0\) 处不可导，需使用**次梯度**判断最优性。函数 h(u) 在 u=0 处的次梯度为：$$\partial h(0) = (0 - a) + c \cdot \partial |0| = -a + c \cdot [-1, 1] = [-a - c,\ -a + c]$$u=0是最优解当且仅当 $0 \in \partial h(0)$，即：$$-a - c \leq 0 \leq -a + c \quad \Leftrightarrow \quad |a| \leq c$$
	 **4. 合并结果**
		 综合三种情况：
			- 若 \(a > c\)，则最优解 \(u = a - c\)（对应 \(u > 0\)）；
			- 若 \(a < -c\)，则最优解 \(u = a + c\)（对应 \(u < 0\)）；
			- 若 $|a| \leq c$，则最优解 \(u = 0\)。
		将 $u = x^i, a = z^i, c = \lambda / L$代回，即得式 (11.14)：$$x_{k+1}^i = \begin{cases} z^i - \lambda / L, & \lambda / L < z^i; \\0, & |z^i| \leq \lambda / L; \\z^i + \lambda / L, & z^i < -\lambda / L.\end{cases}$$
	**5. 几何直观**
		该解也称为 **软阈值函数**（soft-thresholding）：
		- 当$z^i 的绝对值小于阈值 \lambda/L$ 时，系数被“压缩”为零（产生稀疏性）；
		- 否则，系数向零收缩 $\lambda/L$个单位。


## 算法:
[[嵌入式选择与L1正则化的算法]]

# 5.稀疏表示与字典学习
## Introduction
将数据集D考虑为一个矩阵:(行:每个样本,列:每个特征,即我常用的dataframe)
	**特征选择**:特征具有稀疏性,即矩阵中许多列与学习任务无关
		1.可降低训练数据大小,也许能降低训练难度,计算存储开销小
		2.可解释性好(输入--输出关系更清晰)
	现在考虑另一种稀疏,每一个样本中有大量特征为0(eg:文本分类时)
		1.使大多数问题线性可分(支持向量机有很好的性能)
		2.稀疏矩阵有很多高效的存储方式
	而通过 **"字典学习"**,可以将普通数据集转化为这种稀疏表示形式.学习出一个"字典"/"码书"(==B==),将样本转化为合适的稀疏表示形式,从而简化学习难度.(dictionary learning)
	亦称稀疏编码(侧重稍有不同)/码书学习
## 形式:
>![[Pasted image 20251219203010.png]]^dictionary

[[#^LASSO]]  
[[dictionary&LASSO]]
**字典学习其实兼具降维和特征选择的特点**
>![[Pasted image 20251219210108.png]]
## 求解:
正则化项中的$\lambda$就像拉格朗日乘子,用拉格朗日乘子法是可以求导,这里因为L<sub>1</sub>,反过来
>![[Pasted image 20251219210536.png]]
>![[Pasted image 20251219211043.png]]

>[!note]
>**KSVD**:基于逐列更新策略
>1.这里是B矩阵的第i列乘A矩阵的第i行,结果是一个矩阵
>2.每次只优化bi,ai,其余固定,为常数

做到这里,理论上用特征值分解(类似PCA),但是特征值分解要求是方阵且可求逆,因此用**奇异值分解**
>![[Pasted image 20251219195932.png]]
>和$\Lambda$类似,$\sum$上半部分也是一个对角矩阵;然后长方形多出来的下面部分是0 

理论上,对$E_i$进行奇异值分解 以取得 最大奇异值所对应的 正交向量 即可最小化上式
但是,直接对$E_i$进行奇异值分解会同时修改$b_i和a^i$ ,可能破坏**A**的稀疏性.
>![[Pasted image 20251219212633.png]]

==通过$L_1$范数实现稀疏,奇异值分解从大到小取实现降维==
[[降维与度量学习#^PCA-final]]
## example
![[Pasted image 20251219213002.png]]

## 拓展
可以控制字典的结构,比如分组结构.
以汉字为例,可能几个词语表达了一个相同的概念,那就可以把他们分为一组同一个分组内同为0或同为非0.
# 6.压缩感知(仅思想)
## Question
现实中,我们常常希望根据部分信息来恢复全部信息.(eg:数据通讯)
接收方如何基于收到的有损失的信号精确重构原信号?

假定有长度为m的离散信号x,以远小于奈奎斯特采样(**2m**)要求的采样率采样,得到长度为n的y信号,n<<m,即
>![[Pasted image 20251219221003.png]]

现在接收方已知y和$\Phi$ ,需要复原x.  (11.19)为NP难,欠定方程
但是
>![[Pasted image 20251219221304.png]]

这里s具有稀疏性,可以使未知因素的影响大为减少.A就类似于"字典"

## 压缩感知(compressed sensing)
感知测量+重构恢复
感知测量:对原始信号处理获得稀疏样本
重构恢复:**核心**. 基于稀疏性从少量样本恢复原信号

### 限定等距性(RIP)
将$L_0$最后做成一个$L_1$范数优化问题,可用LASSO的基寻踪去噪等方法
### 协同过滤任务(collaborative filtering)
eg:网上书店根据读者评价推荐书
	就可以理解为对读者没有读过(评价)的书的打分进行预测,预测得分高的就可以推荐给读者
	![[Pasted image 20251219222400.png]]
	稀疏直接做,不稀疏通过聚类等方法转换为稀疏
	(比如一个人喜欢的一般是某一两类,那么根据类别,就是稀疏的)
**矩阵补全技术**(matrix completion)可用来解决这个问题*亦称低秩矩阵恢复*