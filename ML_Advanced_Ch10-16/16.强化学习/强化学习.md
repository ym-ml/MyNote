# 1.任务与奖赏
我们考虑如何种西瓜,播种,浇水,施肥...最后种出一个瓜
若将得到好瓜作为最终奖赏, 在过程中每一步进行后,我们并不能立即获得这个奖赏,仅能得到一个反馈(eg:瓜苗看起来更健壮)
我们需多次种瓜,不断摸索,才能总结出经验.此即 **强化学习(reinforcement learning)**

![[Pasted image 20251231153411.png]]
强化学习任务通常用马尔可夫决策过程(Markov Decision Process,**MDP**)来描述:
1. 机器处于环境 $E$ 中
2. 状态空间为 $X$ ,其中每个状态$x \in X$ 是机器感知到的环境的描述.
3. 机器能采取的动作构成动作空间 $A$.
4. 若某个动作 $a \in A$ 作用在状态 $x$上,则潜在的转移函数 $P$ 将使环境从当前状态按某一概率转移到另一状态.
5. 在转移到另一状态的同时,环境会根据潜在的"奖赏"(reward)函数 $R$反馈给机器一个奖赏.
6. 综合起来,强化学习任务对应了四元组 $E=<X,A,P,R>$.

一个给西瓜浇水的简单例子:![[Pasted image 20251231154241.png]]

需注意机器与环境的界限. 
*eg:种西瓜中,环境是西瓜生长的自然世界;下棋中,环境是棋盘和对手.*

环境中状态的转移/奖赏的返回是不受机器控制的,
机器只能通过选择要执行的动作来影响环境,也只能通过观察转移后的状态和返回的奖赏来感知环境.

机器要做的是通过在环境中不断尝试而学得一个策略(policy) $\pi$ ,根据这个策略,在状态 $x$ 下就能得知要执行的动作 $a=\pi (x)$ .
策略有两种表示方法:
1. 函数 $\pi : X \mapsto A$,确定性策略
2. 概率 $\pi : X \times A \mapsto \mathbb R$,随机性策略 $\pi (x,a)$为状态 $x$下选择动作$a$ 的概率,这里必须有**概率和为1**.

策略的优劣取决于长期执行这一策略的累积奖赏.学习的目的就是找到使长期累积奖赏最大化的策略.

长期累计奖赏常用的算法: 
	1. T步累积奖赏: $\mathbb{E}\left[\frac{1}{T}\sum_{t=1}^{T}r_t\right]$
	2. $γ$ 折累积奖赏: $\mathbb{E}\left[\sum_{t=0}^{+\infty}\gamma^t r_{t+1}\right]$ 
其中,$r_t$表示第 t 步获得的奖赏值. ^longtimereward

----
对比:

| 强化学习      | 监督学习    |
| --------- | ------- |
| 状态        | 示例      |
| 动作(连续/离散) | 标记      |
| 策略        | 分类器/回归器 |
不同: 强化学习中没有监督学习中的有标记样本(即"示例-标记"对)
*换言之，没有人直接告诉机器在什么状态下应该做什么动作，只有等到最终结果揭晓，才能通过 "反思"之前的动作是否正确来进行学习*

强化学习在某种意义上可看作具有"延迟标记信息"的监督学习问题.

# 2. K-摇臂赌博机
## 2.1 探索与利用

我们不妨先仅考虑最大化单步奖赏.
*注意,即使只考虑单步,与监督学习仍有和大不同,没有数据告诉机器应当做哪个动作*

欲最大化单步奖赏,考虑两个方面:
1. 知道每个动作带来的奖赏
2. 执行奖赏最大的动作
简单的想法,每个动作都试一下就知道了.
问题在于:一般一个动作的奖赏值是来自于一个概率分布,仅通过一次尝试无法确切获得奖赏期望

单步强化学习任务对应了一个理论模型,"K-摇臂赌博机"(K-armed bandit)
	K-摇臂赌博机有 K 个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币. 但这个概率赌徒并不知道.赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币.
	![[Pasted image 20251231164507.png]]

- 仅探索法:仅为获知每个摇臂的期望奖赏.
	轮流按下各个摇臂,最终将吐出硬币的平均概率作为奖赏期望的近似
- 仅利用法: 仅为执行奖赏最大的动作
	按下目前最优的摇臂.若多个同样,随机选一个
显然,总尝试次数有限, "探索"(估计摇臂的优劣)和"利用"(选择当前最优摇臂)这两者是矛盾的.  
**"探索-利用窘境"**(Exploration-Exploitation dilemma)
显然，欲累积奖赏最大，则必须在探索与利用之间达成较好的折中.

## 2.2 $\epsilon$-贪心

>[!tldr]
>$\epsilon$-贪心法基于一个概率来对探索和利用进行折中:
>每次尝试时,以 $\epsilon$ 的概率进行探索,以 $1-\epsilon$ 的概率进行利用

令 $Q(k)$ 记录摇臂 $k$ 的平均奖赏。若摇臂 $k$ 被尝试了 $n$ 次，得到的奖赏为 $v_1, v_2, \ldots, v_n$，则平均奖赏为: $$Q(k) = \frac{1}{n} \sum_{i=1}^{n} v_i .$$
这个式子不便于计算,我们改写成可增量计算的递推公式:$$
\begin{aligned}
Q_n(k) &= \frac{1}{n} ((n-1) \times Q_{n-1}(k) + v_n)\\
&=Q_{n-1}(k)+ \frac{1}{n}(v_n-Q_{n-1}(k))
\end{aligned}\tag{16.3}$$
算法如图:![[Pasted image 20251231170359.png]]

若当前摇臂奖赏的不确定性较大,(eg:概率分布较宽),则需更多探索,设置较大的$\epsilon$ 值;否则,需要较小的$\epsilon$
当尝试次数很多时,不在需要探索,这种情况可令$\epsilon$ 的值随尝试次数的增加而减少.(eg:令$\epsilon =\frac{1}{\sqrt t}$)

## 2.3 Softmax
>[!tldr]
>基于奖赏的高低为每个摇臂设置一个选取概率
>奖赏高的摇臂选中的概率高

Softmax中摇臂概率的分配是基于Boltzmann分布:$$P(k) = \frac{e^{\frac{Q(k)}{\tau}}}{\sum_{i=1}^{K} e^{\frac{Q(i)}{\tau}}} \tag{16.4}$$
其中, $Q(i)$ 记录当前摇臂的平均奖赏; $\tau>0$称为"温度",$\tau$ 越小,则平均奖励高的摇臂被选中的概率越高.
$\tau$ 趋于0时Softmax将趋于"仅利用", 趋于无穷大时Softmax则将趋于"仅探索"

算法如图![[Pasted image 20251231171924.png]]

## 2.4 补充
1. $\epsilon$-贪心算法与 Softmax算法孰优孰劣，主要取决于具体应用.
	考虑一个简单的例子:假定2-摇臂赌博机的摇臂1 以 0.4 的概率返回奖赏 1，以0.6 的概率返回奖赏0; 摇臂 2 以 0.2 的概率返回奖赏 1， 以 0.8 的概率返回奖赏0。
	不同参数下平均奖赏如图
	>![[Pasted image 20251231195527.png]]
2. 对于离散状态空间、离散动作空间上的**多步**强化学习任务，一种直接的办法是将每个状态上动作的选择看作一个K-摇臂赌博机问题，用强化学习任务的**累积奖赏**来**代替**K-摇臂赌博机算法中的奖赏函数.<br>    当然,这种做法没有考虑马尔可夫决策过程,有很多局限

# 3.有模型学习
考虑多步强化学习任务,暂且假定任务对应的马尔可夫决策过程四元组$E=<X,A,P,R>$均为已知. 
这样的情形称为"模型已知".在已知模型的环境中学习称为"有模型学习"(model-based learning)

此时，对于任意状态 $x, x'$ 和动作 $a$，
- $P_{x\to x'}^a$ :在 $x$ 状态下执行动作 $a$ 转移到 $x'$ 状态的概率
- $R_{x\to x'}^a$ :该转移所带来的奖赏 
二者均已知.
为便于讨论，不妨假设状态空间 $X$ 和动作空间 $A$ 均为有限。

## 3.1策略评估
在模型已知时，对任意策略 $\pi$ 能估计出该策略带来的期望累积奖赏。
令
- 函数 $V^\pi(x)$ 表示从状态 $x$ 出发，使用策略 $\pi$ 所带来的累积奖赏； 
	- $V(\cdot)$ 称为“状态值函数”（state value function）
- 函数 $Q^\pi(x,a)$ 表示从状态 $x$ 出发，执行动作 $a$ 后再使用策略 $\pi$ 带来的累积奖赏。
	- $Q(\cdot)$ 称为“状态-动作值函数”（state-action value function）
这里的$V(\cdot),Q(\cdot)$分别表示指定“状态”上以及指定“状态-动作”上的==累积奖赏==。


![[强化学习#^longtimereward|长期累计奖赏]]
由累计奖赏的定义,有状态值函数:$$
\begin{cases} 
V_T^{\pi}(x) = \mathbb{E}_\pi \left[ \frac{1}{T} \sum_{t=1}^{T} r_t | x_0 = x \right], & T \text{ 步累积奖赏}; \\ 
V_{\gamma}^{\pi}(x) = \mathbb{E}_\pi \left[ \sum_{t=0}^{+\infty} \gamma^t r_{t+1} | x_0 = x \right], & \gamma \text{ 折扣累积奖赏}. 
\end{cases} \tag{16.5}$$
令 $x_0$ 表示起始状态，$a_0$ 表示起始状态上采取的第一个动作；
对于 $T$ 步累积奖赏，用下标 $t$ 表示后续执行的步数。我们有状态-动作值函数:$$
\begin{cases} 
Q_T^{\pi}(x, a) = \mathbb{E}_\pi \left[ \frac{1}{T} \sum_{t=1}^{T} r_t | x_0 = x, a_0 = a \right]; \\ 
Q_{\gamma}^{\pi}(x, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{+\infty} \gamma^t r_{t+1} | x_0 = x, a_0 = a \right]. 
\end{cases} \tag{16.6}$$
马尔可夫决策过程(MDP)具有马尔可夫性,即系统下一时刻状态只由当前时刻状态决定,
所以值函数有很简单的递归形式 *(亦称Bellman等式)*$$
\begin{aligned}
V_T^{\pi}(x) &= \mathbb{E}_\pi \left[ \frac{1}{T} \sum_{t=1}^{T} r_t | x_0 = x \right]\\
&= \mathbb{E}_\pi \left[ \frac{1}{T} r_1 + \frac{T-1}{T} \frac{1}{T-1} \sum_{t=2}^{T} r_t | x_0 = x \right]\\
&= \sum_{a \in A} \pi(x, a) \sum_{x' \in X} P_{x \rightarrow x'}^a \left( \frac{1}{T} R_{x \rightarrow x'}^a + \frac{T-1}{T} \mathbb{E}_\pi \left[ \frac{1}{T-1} \sum_{t=1}^{T-1} r_t | x_0 = x' \right] \right)\\
&= \sum_{a \in A} \pi(x, a) \sum_{x' \in X} P_{x \rightarrow x'}^a \left( \frac{1}{T} R_{x \rightarrow x'}^a + \frac{T-1}{T} V_{T-1}^{\pi}(x') \right). \end{aligned}\tag{16.7}$$
>[!note]
>1. 通过状态转移概率将 $V_{T}^{\pi}$ 和 $V_{T-1}^{\pi}$相联系
>2. 动作-状态全概率展开,即将期望展成了加权平均的形式

类似的,对 $γ$ 折扣累积奖赏,有$$V_{\gamma}^{\pi}(x) = \sum_{a \in A} \pi(x, a) \sum_{x' \in X} P_{x \rightarrow x'}^a (R_{x \rightarrow x'}^a + \gamma V_{\gamma}^{\pi}(x')). \tag{16.8}$$
需注意的是，正是由于 $P$ 和 $R$ 已知，才可以进行全概率展开。

读者可能己发现，用上面的递归等式来计算值函数，实际上就是一种动态规划算法  [failure|暂时不熟动态规划]
- 对于 $V_T^{\pi}$，可设想递归一直进行下去，直到最初的起点；
- 说人话就是，从 $V_0^{\pi}$ 出发，通过一次迭代能计算出每个状态的单步奖赏 $V_1^{\pi}$，进而得到$V_2^{\pi},V_3^{\pi}...$

算法如下
![[Pasted image 20251231203058.png]]
对于 $V_{\gamma}^{\pi}$，由于 $\gamma^t$ 在 $t$ 很大时趋于 0 ($V_{T}^{\pi}$是$\frac{1}{t}$会趋近于0), 因此也能使用类似的算法，只需将图 16.7 算法的第 3 行根据式 (16.8) 进行替换。

此外，由于算法可能会迭代很多次，因此需设置一个停止准则。常见的是设置一个阈值 $\theta$，若在执行一次迭代后值函数的改变小于 $\theta$ 则算法停止；相应的，图 16.7 算法第 4 行中的 $t = T + 1$ 需替换为==小于这个阈值==$$\max_{x \in X} |V(x) - V'(x)| < \theta .
\tag{16.9}$$
有了状态值函数 $V$，就能直接计算出状态-动作值函数
$$\begin{cases}
Q_T^{\pi}(x, a) = \sum_{x' \in X} P_{x \to x'}^a (\frac{1}{T} R_{x \to x'}^a + \frac{T-1}{T} V_{T-1}^{\pi}(x')); \\
Q_\gamma^{\pi}(x, a) = \sum_{x' \in X} P_{x \to x'}^a (R_{x \to x'}^a + \gamma V_\gamma^{\pi}(x')).
\end{cases}
\tag{16.10}$$

## 3.2 策略改进
对某个策略的累积奖赏进行评估后,若发现其非最优,我们想进行改进.
下面讨论如何改进.

一个学习任务可能有多个最优策略
理想的策略应能最大化累积奖励$$\pi^* = \arg \max_{\pi} \sum_{x \in X} V^{\pi}(x). \tag{16.11}$$
最优策略所对应的值函数 $V^*$ 称为最优值函数，即$$\forall x \in X : V^*(x) = V^{\pi^*}(x). \tag{16.12}$$
注意:==累积奖赏值最大的不一定是最优策略,还需满足约束(如果有)==
- 当策略空间无约束时式(16.12)的 $V^*$ 才是最优策略对应的值函数
	- 例如对离散状态空间和离散动作空间，策略空间是所有状态上所有动作的组合，共有 $|A|^{|X|}$ 种不同的策略。
- 若策略空间有约束，则违背约束的策略是“不合法”的，即便其值函数所取得的累积奖赏值最大，也不能作为最优值函数。

由于最优值函数的累积奖赏值已达最大，因此可对前面的 Bellman 等式(16.7)和(16.8)做一个改动，即将对动作的求和改为取最优(最大):
 >$$V_T^{\pi}(x)=\sum_{a \in A} \pi(x, a) \sum_{x' \in X} P_{x \rightarrow x'}^a \left( \frac{1}{T} R_{x \rightarrow x'}^a + \frac{T-1}{T} V_{T-1}^{\pi}(x') \right). \tag{16.7}$$
 >$$V_{\gamma}^{\pi}(x) = \sum_{a \in A} \pi(x, a) \sum_{x' \in X} P_{x \rightarrow x'}^a (R_{x \rightarrow x'}^a + \gamma V_{\gamma}^{\pi}(x')). \tag{16.8}$$
 
$$\begin{aligned}
&V_T^*(x) = \max_{a \in A} \sum_{x' \in X} P_{x \to x'}^a \left( \frac{1}{T} R_{x \to x'}^a + \frac{T-1}{T} V_T^*(x') \right)\\
&V_\gamma^*(x) = \max_{a \in A} \sum_{x' \in X} P_{x \to x'}^a \left( R_{x \to x'}^a + \gamma V_\gamma^*(x') \right).\end{aligned} \tag{16.13}$$
换言之,$$V^*(x) = \max_{a \in A} Q^{\pi^*}(x, a). \tag{16.14}$$
代入式(16.10)可得最优状态-动作值函数$$\begin{cases}
Q_T^*(x, a) = \sum_{x' \in X} P_{x \to x'}^a \left( \frac{1}{T} R_{x \to x'}^a + \frac{T-1}{T} \max_{a' \in A} Q_T^*(x', a') \right); \\
Q_\gamma^*(x, a) = \sum_{x' \in X} P_{x \to x'}^a \left( R_{x \to x'}^a + \gamma \max_{a' \in A} Q_\gamma^*(x', a') \right).
\end{cases} \tag{16.15}$$
上述关于最优值函数的等式,称为最优Bellman等式, 其唯一解是最优值函数

----
<span style="color:#6B8CBE">我们推出了目标,现在进行分析:</span>

最优 Bellman 等式揭示了非最优策略的改进方式：*将策略选择的动作改变为当前最优的动作*。

显然，这样的改变能使策略更好。
我们进行**证明**:
不妨令动作改变后对应的策略为 $\pi'$，改变动作的条件为 $Q^\pi(x, \pi'(x)) \geq V^\pi(x)$，以 $\gamma$ 折扣累积奖赏为例，由式(16.10)可计算出递推不等式
$$\begin{aligned} V^\pi(x) 
&\leq Q^\pi(x, \pi'(x))\\
&= \sum_{x' \in X} P_{x \to x'}^{\pi'(x)} (R_{x \to x'}^{\pi'(x)} + \gamma V^\pi(x'))\\
&\leq \sum_{x' \in X} P_{x \to x'}^{\pi'(x)} (R_{x \to x'}^{\pi'(x)} + \gamma Q^\pi(x', \pi'(x')))\\
&= \cdots\\
&= V^{\pi'}(x).\end{aligned}\tag{16.16}$$
即: $V^\pi(x) \le V^{\pi'}(x)$
可以得到,值函数对于策略的每一点改进都是单调递增的.因此对于当前策略 $\pi$，可放心地将其改进为$$\pi'(x) = \arg \max_{a \in A} Q^\pi(x, a), \tag{16.17}$$
直到 $\pi'$ 与 $\pi$ 一致、不再发生变化，此时就满足了最优 Bellman 等式，即找到了最优策略。

----
>[!attention]
>Q: 1,2节这里$Q(\cdot)$在干什么,为什么先执行一步动作 $a$ 再按照策略 $\pi$ 
>
>A: $Q(\cdot)$和$V(\cdot)$的差别就在走的这一步 $a$ 上,所以可以用$Q(\cdot)$来评估/改进策略
>
>就是说如果我这一步没有按$\pi$走,而是做了别的动作,如果效果比按策略走好,说明策略不行,还需优化.  
>而算法中我们尝试了所有的$a$ ,即$a \in A$.
>
>而改变动作的条件$Q^\pi(x, \pi'(x)) \geq V^\pi(x)$  意味着第一步按照$\pi'$ 的策略走效果要优于按$\pi$ .所以优化


## 3.3策略迭代与值迭代
### 策略迭代
前两小节已经讨论了如何进行评估和改进值函数,那么我们只需评估--改进--再评估--在改进...不断迭代直到收敛即可
这样的做法就是"策略迭代"(policy iteration)

算法如图![[Pasted image 20251231214703.png]]

类似的, 可得到基于 $γ$折扣累积奖赏的策略迭代算法

策略迭代算法在每次改进策略后都需重新进行策略评估，这通常比较耗时.
so,有值迭代
### 值迭代
观察式(16.14),(16.17),我们可以发现==策略的改进和值函数的改进是一致的==
>$$\begin{aligned}
&V^*(x) = \max_{a \in A} Q^{\pi^*}(x, a)\\
&\pi'(x) = \arg \max_{a \in A} Q^\pi(x, a)\\
\end{aligned}$$

所以由式(16.13)可得:$$\begin{cases} 
V_T(x) = \max_{a \in A} \sum_{x' \in X} P^a_{x \to x'} \left( \frac{1}{T} R^a_{x \to x'} + \frac{T-1}{T} V_{T-1}(x') \right); \\ 
V_\gamma(x) = \max_{a \in A} \sum_{x' \in X} P^a_{x \to x'} \left( R^a_{x \to x'} + \gamma V_\gamma(x') \right). 
\end{cases}
\tag{16.18}$$
可得优化后的算法:![[Pasted image 20251231215618.png]]

若采用$\gamma$折扣累积奖励,只需将第3行换成$\forall x\in X:V_\gamma(x) = \max_{a \in A} \sum_{x' \in X} P^a_{x \to x'} \left( R^a_{x \to x'} + \gamma V_\gamma(x') \right);$

----
从上面的算法可看出, 在模型已知时强化学习任务能归结为**基于动态规划**的**寻优问题**. 
与监督学习不同,这里并未涉及到泛化能力，而是为每一个状态找到最好的动作.

# 4.免模型学习
在现实的强化学习任务中,转移概率/奖赏函数/总状态 等很难知道. 而若学习算法不依赖于这些建模, 就叫"**免模型学习**"(model-free learning)

>[!note]
>理解:
>免模型学习 & 有模型学习
>- 免模型不代表我无法得到奖励/无法转到下个状态. 我只要做了动作,就会到下一个状态.
>- 有模型学习就像开了上帝视角.
>
>用走迷宫为例:
>- 前面有模型学习就是我知道允许的走的方向,知道拐到每个路口的成功的概率是多少,知道拐的奖励. 需要做的就是动态规划的**寻优**问题,找出一条最优的线路
><br>
>- 免模型学习就是我的视野是黑的,我可以按照策略选择往哪走,而每走一步之后我的状态发生变化,会得到一个反馈,基于这个反馈得到奖励.*(比如撞墙了,扣分; 没事,加一分; 没路了,扣100分)*.最后就会得到一个执行 $T$ 步的轨迹.

## 4.1蒙特卡罗强化学习
### 免模型的问题&解决
| 问题                                            | 解决                                                                           | 备注                                             |
| --------------------------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------- |
| 策略无法评估:<br>模型未知,无法像前面有模型做全概率展开                | 通过在环境中执行选择的动作,观察转移的状态和得到的奖赏<br><br>**蒙特卡罗强化学习**: 多次"采样"(一次得到一条轨迹),然后求取平均累积奖赏 | 1.蒙..是受K摇臂赌博机启发<br>2.采样必须为有限次,所以更适于使用T步累计奖赏的任务 |
| 策略迭代算法估计的是状态值函数 $V$,但是模型未知时从$V$不容易到$Q$        | 直接估计状态-动作值函数$Q$                                                              |                                                |
| 模型未知,机器只能从一个起始状态(or状态集合)开始,但是策略迭代算法需对每个状态分别估计 | 从起始状态出发探索，逐步发现并估计各状态-动作对                                                     |                                                |
综合起来，在模型未知的情形下，我们从起始状态出发、使用某种策略进行采样，执行该策略T步并获得轨迹$$<x_0,a_0,r_1,x_1,a_1,r_2,...,x_{T-1},a_{T-1},r_{T-1},x_T>$$
然后,  对轨迹中出现的每一对状态-动作, 记录其后的奖赏之和，作为该状态-动作对的一次累积奖赏采样值.    *即记录 $\sum_{r}$*
多次采样得到多条轨迹,之后对累积奖赏平均

### $\epsilon$-贪心(采样策略与探索利用平衡)

我们的策略是确定的,若使用相同的策略采样,只会得到相同的轨迹. 
这与K摇臂赌博机的"仅利用"的问题一样.
因此可借鉴 "探索于利用折中" 的办法:
- 采用 **ε-贪心策略** 进行探索：
    - 以 $1−ε$ 概率选择当前最优动作
    - 以 $ε$ 概率随机选择动作（均匀分布）
- 保证每个动作都有机会被尝试，从而获得多样本轨迹
	- 当前最优动作被选中的概率是 $1 - \epsilon + \frac{\epsilon}{|A|}$
	- 每个非最优动作被选中的概率是 $\frac{\epsilon}{|A|}$


将确定性策略 $\pi$ 称为"原始策略"
在原始策略上使用 $\epsilon$-贪心的策略记为$$
\pi^\epsilon(x)=
\begin{cases}
&\pi (x),\;以概率1-\epsilon \\
&A中以均匀概率选取的动作,\; 以概率\epsilon
\end{cases}$$

### "同策略"(on-policy)蒙特卡罗强化学习算法
<span style="color:#6B8CBE">与策略迭代算法类似，使用蒙特卡罗方法进行策略评估后，同样要对策略进行改进</span>

前面在讨论策略改进时利用了式(16.16)揭示的单调性，通过换入当前最优动作来改进策略。

对于任意原始策略 $\pi$，其 $\epsilon$-贪心策略 $\pi^\epsilon$ 仅是将 $\epsilon$ 的概率均匀分配给所有动作，因此对于最大化值函数的原始策略 $\pi'$，同样有 $Q^\pi(x, \pi'(x)) \geq V^\pi(x)$，于是==式(16.16)仍成立==，即可以使用同样方法来进行策略改进。

这里被评估与被改进的是同一个策略，因此称为“同策略”(on-policy)蒙特卡罗强化学习算法。
 *注:*
 1. *算法中奖赏均值采用增量式计算*
 2. *奖赏是累积的*
 ![[Pasted image 20260101210917.png]]
### "异策略"蒙特卡罗强化学习算法
事实上,引入 $\epsilon$-贪心是为了评估,我们希望改进的是原始策略.
那么能否评估时用 $\epsilon$-贪心, 改进时改进原始呢?
当然

==重要性采样(importance sampling):==基于一个分布的采样来估计另一个分布下的期望.

----
#### 推导
不妨用两个不同的策略 $\pi$ 和 $\pi'$ 来产生采样轨迹.两者的区别在于每个"状态-动作对"被采样的概率不同.
一般的，函数 $f$ 在概率分布 $p$ 下的期望可表达为式(16.21)。$$E[f] = \int_{x} p(x) f(x) dx, \tag{16.21}$$

可通过从概率分布 $p$ 上的采样 ${x_1, x_2, \ldots, x_m}$ 来估计 $f$ 的期望，即式(16.22)。$$\hat{E}[f] = \frac{1}{m} \sum_{i=1}^{m} f(x). \tag{16.22}$$

若引入另一个分布 $q$，则函数 $f$ 在概率分布 $p$ 下的期望也可等价地写为式(16.23)。$$ E[f] = \int_{x} q(x) \frac{p(x)}{q(x)} f(x) dx. \tag{16.23}  $$

上式可看作 $\frac{p(x)}{q(x)} f(x)$ 在分布 $q$ 下的期望，因此通过在 $q$ 上的采样 ${x_1', x_2', \ldots, x_m'}$ 可估计为式(16.24)。$$ \hat{E}[f] = \frac{1}{m} \sum_{i=1}^{m} \frac{p(x_i')}{q(x_i')} f(x_i'). \tag{16.24}  $$

回到我们的问题上来，使用策略 $\pi$ 的采样轨迹来评估策略 $\pi$，实际上就是对累积奖赏估计期望式(16.25)。$$ Q(x,a) = \frac{1}{m} \sum_{i=1}^{m} r_i. \tag{16.25}  $$

若改用策略 $\pi'$ 的采样轨迹来评估策略 $\pi$，则仅需对累积奖赏加权，即式(16.26)，$$ Q(x,a) = \frac{1}{m} \sum_{i=1}^{m} \frac{P_i^{\pi}}{P_i^{\pi'}} r_i, \tag{16.26}  $$

其中 $P_i^{\pi}$ 和 $P_i^{\pi'}$ 分别表示两个策略产生第 $i$ 条轨迹的概率。对于给定的一条轨迹 $\langle x_0, a_0, r_1, \ldots, x_{T-1}, a_{T-1}, r_T, x_T \rangle$，策略 $\pi$ 产生该轨迹的概率为式(16.27)。$$ P^{\pi} = \prod_{i=0}^{T-1} \pi(x_i, a_i) P_{x_i \to x_{i+1}}^{a_i}. \tag{16.27}  $$

显然这里用到了环境的转移概率 $P_{x_i \to x_{i+1}}^{a_i}$，但式(16.24)中实际只需两个策略概率的比值式(16.28)。
$$ \frac{P^{\pi}}{P^{\pi'}} = \prod_{i=0}^{T-1} \frac{\pi(x_i, a_i)}{\pi'(x_i, a_i)}. \tag{16.28}  $$

若 $\pi$ 为确定性策略而 $\pi'$ 是 $\pi$ 的 $\epsilon$-贪心策略，则 $\pi(x_i, a_i)$ 始终为 1，$\pi'(x_i, a_i)$ 为 $\frac{\epsilon}{|A|}$ 或 $1 - \epsilon + \frac{\epsilon}{|A|}$，于是就能对策略 $\pi$ 进行评估了。
#### 结论:
推导出来就是做了一个加权平均,(16.28)带入(16.26),得到最终用来计算的式子$$Q(x,a) = \frac{1}{T} \sum_{j=1}^{T} \prod_{i=0}^{T-1} \frac{\pi(x_i, a_i)}{\pi'(x_i, a_i)} r_j, \tag{16.00}$$

图 16.11 给出了“异策略”(off-policy)蒙特卡罗强化学习算法的描述。式(16.00)和图中第六行等价,第六行中对求和与求积进行了整理,且为了增量计算,用$R$来表示
![[Pasted image 20260101214454.png]]

## 4.2时序差分学习

前面介绍的基于动态规划的策略迭代/值迭代 在每执行一步之后就能更新一次
**蒙特卡罗**强化学习的问题:采样一个轨迹才能进行一次策略更新,**效率**非常低下.
主要问题:没有充分利用强化学习任务的MDP结构

**时序差分(Temporal Difference,TD)学习**则结合了动态规划与蒙特卡罗方法的思想

>[!tldr]
>将蒙特卡罗强化学习改进成增量式的

**蒙特卡罗强化学习算法的本质:通过多次尝试后求平均来作为期望累积奖赏的近似.** 但是它在求平均时是"批处理式"进行的,即在一个完整的采样轨迹完成后再对所有的状态-动作对进行更新.

实际上,我们可以增量式进行.
对于状态-动作对 $(x,a)$，不妨假定基于 $t$ 个采样已估计出值函数 $Q_t^T(x,a) = \frac{1}{t} \sum_{i=1}^t r_i$，则在得到第 $t+1$ 个采样 $r_{t+1}$ 时，类似式(16.3)，有$$Q_{t+1}^T(x,a) = Q_t^T(x,a) + \frac{1}{t+1} (r_{t+1} - Q_t^T(x,a)).\tag{16.29}$$

>[!note]
>式(16.29)就是 $$\bar{x}_{n+1} =\frac{\bar{x}\times n +x_{n+1}}{n+1}$$


显然，只需给 $Q_t^T(x,a)$ 加上增量 $\frac{1}{t+1}(r_{t+1} - Q_t^T(x,a))$ 即可。更一般的，将 $\frac{1}{t+1}$ 替换为系数 $\alpha_{t+1}$，则可将增量项写作 $\alpha_{t+1}(r_{t+1} - Q_t^T(x,a))$。在实践中通常令 $\alpha_t$ 为一个较小的正数值 $\alpha$，若将 $Q_t^T(x,a)$ 展开为每步累积奖赏之和，则可看出系数之和为1，即令 $\alpha_t = \alpha$ 不会影响 $Q_t$ 是累积奖赏之和这一性质。更新步长 $\alpha$ 越大，则越靠后的累积奖赏越重要。
>[!failure]
>这一段没看太懂

----
以 $\gamma$ 折扣累积奖赏为例，利用动态规划方法且考虑到模型未知时使用状态-动作值函数更方便，由式(16.10, 写在了第一行)有$$
\begin{aligned}
Q^\pi(x,a) &= \sum_{x' \in X} P_{x \to x'}^{a} (R_{x \to x'}^{a} + \gamma V^\pi(x')) \\
&= \sum_{x' \in X} P_{x \to x'}^{a} (R_{x \to x'}^{a} + \gamma \sum_{a' \in A} \pi(x', a') Q^\pi(x', a')).
\end{aligned}
\tag{16.30}$$
通过增量求和可得
$$Q_{t+1}^T(x,a) = Q_t^T(x,a) + \alpha (R_{x \to x'}^{a} + \gamma Q_t^T(x', a') - Q_t^T(x,a))\tag{16.31}$$
其中 $x'$ 是前一次在状态 $x$ 执行动作 $a$ 后转移到的状态，$a'$ 是策略 $\pi$ 在 $x'$ 上选择的动作。

----
使用式(16.31)，每执行一步策略就更新一次值函数估计，于是得到图16.12的算法。该算法由于每次更新值函数需知道前一步的状态(state)、前一步的动作(action)、奖赏值(reward)、当前状态(state)、将要执行的动作(action)，由此得名为**Sarsa算法**[Rummery and Niranjan, 1994]。显然，Sarsa是一个同策略算法，算法中评估(第6行)、执行(第5行)的均为$\epsilon$-贪心策略。

将Sarsa修改为异策略算法，则得到图16.13描述的Q-学习(Q-learning)算法[Watkins and Dayan, 1992]，该算法评估(第6行)的是$\epsilon$-贪心策略，而执行(第5行)的是原始策略。

![[Pasted image 20260101221017.png]]
# 5.值函数近似

前面我们一直假设强化学习任务是在有限状态空间上进行,每个状态可以用一个编号代替;值函数可以写做对应的数组形式,即"表格值函数".

然而,现实中常常状态空间连续,无穷,怎么做?
一种直接的想法是对状态空间离散化.但是...不好做

----
我们尝试直接对连续状态空间的值函数进行学习.先假定值函数能表示成关于状态的线性函数.$$V_\theta(\boldsymbol x)=\boldsymbol \theta^T \boldsymbol x \tag{16.32}$$
由于此时值函数很难像之前一样精确记录每个状态的值,所以叫"**值函数近似(value function approximation)"**

----
我们希望通过上式学得的值函数尽可能接近真实值函数 $V^\pi$.近似程度常有最小二乘误差度量:$$E_\theta = \mathbb E_{x \sim \pi} \left[ \left( V^\pi(x) - V_\theta(x) \right)^2 \right] \tag{16.33}$$
其中$\mathbb E_{x \sim \pi}$表示由策略 $\pi$ 采样而得的状态上的期望


最小化误差, 采用梯度下降法,求负导数:$$-\frac{\partial E_\theta}{\partial \theta} = \mathbb E_{x \sim \pi} \left[ 2 \left( V^\pi(x) - V_\theta(x) \right) \frac{\partial V_\theta(x)}{\partial \theta} \right] =\mathbb E_{x \sim \pi} \left[ 2 \left( V^\pi(x) - V_\theta(x) \right) x \right] \tag{16.34}$$

可得到对单个样本的$\boldsymbol \theta$更新规则:$$\theta = \theta + \alpha \left( V^\pi(x) - V_\theta(x) \right) x \tag{16.35}$$

----
但是我们不知道真实值函数 $V^\pi$,可以借助时序差分学习,基于$V^\pi(x)=r+\gamma V^\pi(x')$,用当前估计的值函数代替真实的值函数.即$$\begin{aligned}\theta 
&= \theta + \alpha (r + \gamma V_\theta(x') - V_\theta(x)) x \\
&= \theta + \alpha (r + \gamma \theta^T x' - \theta^T x) x \\
\end{aligned}\tag{16.36}$$

但是用时序差分学习需要有状态-动作值函数来获取策略
所以我们可以
1. 令$\boldsymbol \theta$ 作用于表示状态和动作的联合向量上.eg:给状态向量增加一维用于存放动作的编号,即将$\boldsymbol x$替换为$(\boldsymbol x;a)$
2. 用0/1 实现对动作的选择,构造向量$\boldsymbol a =(0;0;...;0;1;0;...;0)$,在将该向量和状态向量合并,得$(\boldsymbol x;\boldsymbol a)$

基于线性值函数近似来替代Sarsa算法中的值函数,即可得到图16.14中的线性值函数近似Sarsa算法
![[Pasted image 20260102115020.png]]
注:  
- 5为原始策略的 $\epsilon$-贪心策略. 
- 6为式(16.36)更新参数.

类似的,可得到Q-学习算法
还可以引入核方法,进行非线性近似


# 6.模仿学习
强化学习的经典任务设置中,机器所能得到的反馈信息仅有多步决策后的累积奖赏.
而现实任务中,往往能得到人类专家的决策过程范例.
从范例中学习,即为 **"模仿学习"(imitation learning)**

## 6.1直接模仿学习

>[!tldr]
>直接模仿人类专家的"状态-动作对"

强化学习中多步决策的搜索空间巨大,通过奖赏学习很多步之前的合适决策非常困难,而直接模仿人类专家的"状态-动作对"可显著缓解这一困难.

假定我们获得了一批人类专家的决策轨迹数据$\{\tau_1,\tau_2,...,\tau_m\}$,每条轨迹包含动作和序列
$$\tau_i = \langle s_1^i, a_1^i, s_2^i, a_2^i, \ldots, s_{n_i+1}^i \rangle$$

有了这些数据就相当于告诉机器在什么状态下应该做什么动作,我们可以利用监督学习的方法来学.
将所有轨迹上的状态动作对抽取出来 ,构成==以状态为特征,以动作为标签==的新数据集合
$$D = \{(s_1, a_1), (s_2, a_2), \ldots, (s_{\sum_{i=1}^{m} n_i}, a_{\sum_{i=1}^{m} n_i})\},$$
用回归/分类学习一个策略,这个策略可以作为初始策略,然后再用前面讲到的方法基于环境反馈进行改进

## 6.2逆强化学习(inverse RL)
>[!tldr]
>借助人类专家的数据去反推奖励函数
>
>使在这个奖赏函数下的最优策略和范例数据一致


逆强化学习中,我们知道状态空间 $X$ ,动作空间 $A$ ,有一个专家的决策轨迹数据集$\{\tau_1,\tau_2,...,\tau_m\}$.
<span style="color:red">逆强化学习基本思想:</span>欲使机器做出与范例一致的行为,等价于在某个奖赏函数的环境中求解最优策略使该最优策略所产生的轨迹与范例数据一致.

不妨设奖赏函数能表达为状态特征的线性函数, 即$R(\boldsymbol x)=\boldsymbol w^T \boldsymbol x$.于是策略 $\pi$ 的**累积奖赏**可写为$$ 
\rho^\pi = E \left[ \sum_{t=0}^{+\infty} \gamma^t R(\boldsymbol x_t) | \pi \right] = E \left[ \sum_{t=0}^{+\infty} \gamma^t \boldsymbol w^T \boldsymbol x_t | \pi \right] = \boldsymbol w^T E \left[ \sum_{t=0}^{+\infty} \gamma^t \boldsymbol x_t | \pi \right], \tag{16.37}
 $$
$\color{green}即状态向量加权和的期望与系数 \boldsymbol w 的内积$

将状态向量的期望 $\mathbb E \left[ \sum_{t=0}^{+\infty} \gamma^t \boldsymbol x_t | \pi \right]$ 简写为 $\bar{\boldsymbol x}^\pi$。
注意到获得 $\bar{\boldsymbol x}^\pi$ 需求取期望。我们可使用**蒙特卡罗方法**通过采样来近似期望，而**范例轨迹**数据集恰可看作最优策略的一个**采样**，于是，可将每条范例轨迹上的状态加权求和再平均，记为 $\bar{\boldsymbol x}^*$。

对于最优奖赏函数 $R(\boldsymbol x) = \boldsymbol w^{_T} \boldsymbol x$ 和任意其他策略产生的 $\bar{\boldsymbol x}^\pi$，有$$ 
\boldsymbol w^{*T} \bar{\boldsymbol x}^* - \boldsymbol w^{*T} \bar{\boldsymbol x}^\pi = \boldsymbol w^{*T} (\bar{\boldsymbol x}^* - \bar{\boldsymbol x}^\pi) \geq 0. \tag{16.38}
 $$
$\color{green}范例是最优的, 所以累积奖赏最大$

若能对所有策略计算出$(\bar{\boldsymbol x}^* - \bar{\boldsymbol x}^\pi)$，即可解出$$ 
\begin{aligned}
\boldsymbol w^* &= \arg \max_{\boldsymbol w}\; \min_{\pi} \boldsymbol w^T (\bar{\boldsymbol x}^* - \bar{\boldsymbol x}^\pi) \\
&\text{s.t. } \|\boldsymbol w\| \leq 1
\end{aligned} \tag{16.39}
 $$

$\color{green} 即求使式(16.38)在策略 \pi 下取值最小的最大的 \boldsymbol w$
显然，我们难以获得所有策略，一个较好的办法是从随机策略开始，迭代地求解更好的奖赏函数，基于奖赏函数获得更好的策略，直至最终获得最符合范例轨迹数据的奖赏函数和策略，如图16.15算法所示。
*注意*
- *在求解更好的奖赏函数时，需将式(16.39)中对所有策略求最小改为对之前学得的策略求最小.(即图16.15的算法中的第5行 $\min_{i=1}^t$)*
![[Pasted image 20260102112640.png]]