## 1.

Import torch

torch语法类似numpy

torch.tensor==np.array

## 2.

转换:

torch(X)-->np :arr=X.numpy()

np-->torch  torch.tensor(arr)

## 3.

X.clone()  ==  arr.copy()

## 4.

|   |   |
|---|---|
|X.shape|>>>(2,3,5)|
|X.sum(axis=(1,2)).shape|>>>torch.Size([2])|

## 5.

numpy and torch have  "keepdims=True" to keep the axis when use sum,mean…

eg:(2,3,5)   sum(axis=0)-->(3,5)

   sum(axis=0,keepdims=True)-->(1,3,5)

it's used to broadcast

## 6.一个向量x,通常默认是列向量

点积:torch.dot两个向量相同位置元素乘积的和(又叫内积)XTX-->数

外积:XXT-->矩阵

矩阵-向量积:torch.mv(A,x) A的列维数等于x的长度

矩阵-矩阵乘法:A的每一行和B的每一列相乘  torch.mm(A,B)

np @/matmul

torch @/mm

np:dot 对二维数组也可,一维数组和@/matmul会不同;  torch:dot 只能对一维(知道即可)

范数(norm):刻画矩阵\向量的大小

torch.norm(X):L2范数(欧氏距离)

L1范数:绝对值的和

## 7.自动求导

首先创建自变量x的时候需要x=torch.arange(4.0,requirs_grad=True)或者

x=… then

x.requirs_grad_(True)  (应该是对象的属性)

y.backward()得到梯度之后需要清理:x.grad=None or x.grad.zero_()

|   |   |
|---|---|
|对标量:|X=[0,1,2,3]<br><br>y=torch.dot(x,x)<br><br>y.backward()<br><br>即可得到梯度:x.grad        x.grad==4*x (4*X是求导)|
|对向量:|y.backward()需要一个gradient参数,gradient是一个和y 长度相同的向量,并且实际上是<br><br>1.将向量 y 与传入的 gradient 参数(权重)进行点积，得到一个标量<br><br>2.然后对这个标量求梯度<br><br># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。  <br># 本例只想求偏导数的和，所以传递一个1的梯度是合适的  <br>x.grad.zero_()  <br>y = x * x  <br># 等价于y.backward(torch.ones(len(x)))  <br>y.sum().backward()  <br>x.grad<br><br>tensor([0., 2., 4., 6.])<br><br>来自 <[https://zh.d2l.ai/chapter_preliminaries/autograd.html](https://zh.d2l.ai/chapter_preliminaries/autograd.html)>|

  在进行了一次自动求导z.backward()之后,torch默认**清空计算图**中的中间变量(梯度信息\中间结果),所以然后再次调用z.backward()会报错.可以用z.backward(return_garph=True)保存
  