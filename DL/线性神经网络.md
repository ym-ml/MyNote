# 3.1线性回归
## 正态分布与平方损失
为什么用平方损失:<br>
- 假设噪声服从正态分布,有$y=\boldsymbol w^T \boldsymbol x +b+\epsilon$,其中,$\epsilon \sim N(0,\sigma^2)$.
- 则对于给定的$\boldsymbol x$我们可以得到y的似然:$$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).$$
- 最小化负对数似然,可得$$-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.$$
- 假设标准差$\sigma$是某个固定的常数,可以发现需要优化的部分就和均方误差差了一个分母常数.

# 3.4 softmax回归
## 3.4.1分类问题
softmax处理分类问题.标签为**独热编码** eg: $y=\{0,1,0\}$

向量表示为: $\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$

单隐层,全连接.<br>
若有d个输入和q个输出,则参数开销为$O(dq)$.幸运的是,我们可以指定超参数n,将成本降低到$O(dq/n)$.<br>
在参数节省和模型有效性之间权衡.
## 3.4.2 softmax运算
我们需要将输出转换成属于各个类别的概率.<br>
对输出$\{o_1,o_2,...,o_i\}$,有
$$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$$
这样可以确保输出的结果是概率

在实际运算中,是对矩阵$\mathbf{O}$每个元素用`exp`,然后除以每一行元素的和
```python
>>>import numpy as np
>>>arr=np.array([[1,2,0.5],[1.1,0.3,-0.8]])
>>>arre=np.exp(arr)
>>>arre/arre.sum(axis=1,keepdims=True)
array([[0.2312239 , 0.62853172, 0.14024438],
       [0.62543093, 0.28102423, 0.09354484]])
```
## 3.4.3 损失函数
*注:*
1. *$log$ 实际上是 $ln$ ,以 $e$ 为底数*
2. *假设整个数据集有 $n$ 个样本,$y$ 是长度为 $q$ 的独热编码向量*
### 1.对数似然

似然:$$P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).$$
最小化负对数似然:$$-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),$$
损失函数为*交叉熵损失(cross-entropy loss)*:$$l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j.$$

----
### 2.softmax及其导数
将softmax得到的$\hat y$代入损失函数,得到:
$$\begin{split}\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}\end{split}$$
对$o_j$求导,得:$$\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.$$
可以看到,导数是我们softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异.

在任何指数族分布模型中（参见本书附录中关于数学分布的一节），对数似然的梯度正是由此得出的。

## 3.4.4 信息论基础
信息论(information theory)涉及编码、解码、发送以及尽可能简洁地处理信息或数据。
### 1.熵
信息论的核心是量化数据中的信息内容.该数值被称为分布 $P$ 的熵(entropy).$$H[P] = \sum_j - P(j) \log P(j).$$
为了对从分布 $P$ 中随机抽取的数据进行编码，我们至少需要 $H[P]$ “纳特（nat）”对其进行编码。 “纳特”相当于比特（bit），但是对数底为 $e$ 而不是2。
### 2.信息量
压缩和预测的关系:如果我们易于预测下一个数据,那这个数据就容易压缩<br>
但是我们无法完全预测每个事件,有时可能会"惊异"<br>

香农决定用信息量$\log \frac{1}{P(j)} = -\log P(j)$来量化这种惊异程度<br>
在观察一个事件 $j$ 时，并赋予它（主观）概率$P(j)$。当我们赋予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。

在 (3.4.11)中定义的**熵**，是当分配的概率真正匹配数据生成过程时的**信息量的期望。**
### 3.重新审视交叉熵
我们可以把交叉熵想象为“主观概率为的观察者在看到根据概率生成的数据时的预期惊异”。
<div style="background: #f2cac9; border-left: 5px solid #ed5126; padding: 10px; margin: 10px 0;">
<strong>ATTENTION</strong><br>
模型实际上就是对数据的压缩<br><br>    
我们希望通过我们的模型赋予每一个样本在每一个类别下合适的概率 $\hat y_j$,使得我们的"惊异"在实际分布 $y_j$ 下最小.这样就训练出了合适的模型
</div>
简而言之，我们可以从两方面来考虑交叉熵分类目标：<br>
（i）最大化观测数据的似然；（ii）最小化传达标签所需的惊异。
