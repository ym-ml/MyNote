# 10.1 k邻近学习(KNN)
## 1.过程
给定测试数据,在训练集中寻找与它**最近**的k个'邻居',根据这k个样本进行预测.(分类:投票,回归:平均,可以基于距离远近加权)
	**不同:** '懒惰学习'(lazy learning),训练只是把数据存起来,时间开销为0.
## 2.核心:
**距离**和**k
## 3.性能:

以k=1为例,x:测试样本 z最近邻样本
>![[Pasted image 20251213093452.png]]
>
>![[Pasted image 20251213094227.png]]

==就是贝叶斯最优分类器会把这个x分到c^\*==


>![[Pasted image 20251213093632.png]]
x,z很近,所以P(c|x)=\=P(c|y) 
**k邻近的泛化错误率小于贝叶斯最优分类器错误率的2倍

## 10.2 低维嵌入

### 1.why?
前面的讨论基于测试样本总能在任意近的范围内找到z,但是当维度很高的时候,会出现样本稀疏,距离计算困难的问题,也是一个ML共同的问题----维数灾难(curse of dimensionality)
### 2.缓解方法
 1)降维/维数约简 :有些维度与结果关系并不密切
 2)特征选择(下一章)
**2.1'多维缩放'(MDS)**
	去寻找一种映射,使得d维空间中每个点<span style="color:red">在d'维中的欧氏距离等于原始空间中的距离</span>

[[MDS推导.excalidraw]]

**2.2 线性降维**
	![[Pasted image 20251213105440.png]]
	<span style="color:red">这里一列是一个样本</span>,将**X**从==d\*m==变成==d'\*m==
	简单来说就是把不需要的维度通过w设置成0来抹去<br>
	通常令d'<\<d
**2.3:评估**
	1.在测试集上评估
	2.对3d--2d,可以利用可视化,直观判断
## 10.3主成分分析(PCA)

(Principal Component Analysis),属于**线性降维** Z=W<sup>T</sup>X

==找一个超平面,来表达所有样本.这个超平面应该具有性质:
1.样本点到这个超平面的距离都足够近(**最近重构性**)
2.样本点在这个超平面上的投影尽可能分开(**最大可分性**)== (都挤在一起很难分类)
这两种推导等价

### 1)基于最大可分性:

#### 1.推导
最大化Z每一行的方差<span style="color:red">(Z的每一列代表一个样本)</span>

>![[Pasted image 20251213135016.png]]

*Z每一行的方差 求和*
*均值z bar是0(已经**normalised**),忽略      **\*\*** 或者可以只中心化,就是减去平均值*

*观察可得,最后的结果实际上是ZZ<sup>T</sup>的对角线,即tr(ZZ<sup>T</sup>)  (**tr:矩阵的迹**,在后面求导的时候很有用)*

*因为结果可能有很多,所以这里限制了=**I**,即正交*

拉格朗日乘子法:
>![[Pasted image 20251213140103.png]]

*W:d\*d'*
*则 W<sup>T</sup>W=d'\*d'*

求导:
>![[Pasted image 20251213140440.png]]

*拆出来的这d'个式子符合特征值特征向量的定义*

>![[Pasted image 20251213141036.png]]

*即**只需要对协方差矩阵XX<sup>T</sup>进行特征值分解,将求得的特征值从大到小排序,选前d'个即可***
*实践中,常通过对X进行**奇异值分解**来代替协方差矩阵的特征值分解*
^PCA-final

#### 2.选择d'
1.用户事先指定
2.对kNN(或其他开销小的)在不同d'之下做交叉验证
3.对PCA,可以设置一个重构阈值
>![[Pasted image 20251213141941.png]]

**注:** 舍弃d-d',一方面降维,另一方面,可以去噪声

## 10.4 核化线性降维
### 1.why?
>![[Pasted image 20251213155928.png]]
### 2.how
常用:基于核技巧,以核主成分分析(KPCA)
将原样本空间映射到高维,从而实现可以线性降维.
**推导**
>![[Pasted image 20251213163603.png]]
>![[Pasted image 20251213163622.png]]
>**5. 新样本投影坐标**  
>对于新样本 $\boldsymbol{x}$，其第 $j$ 维坐标为：
>![[Pasted image 20251213163729.png]]
## 10.5 流形学习(manifold learning)
**"流形"**":在局部与欧氏空间同胚的空间
>[!note]
**核心**:在局部建立降维映射关系,再设法推广到全局

### 10.5.1等度量映射(Isomap)
>![[Pasted image 20251215151122.png]]

红色的线:测地线距离(本真距离),我们用局部的一个一个小的线段来近似这个曲线,然后再降维
**(主要目的是降维,要解决的问题是怎么降维,怎么做降维之后的距离)**
#### 1)训练 详细描述:
对**每个点**,基于欧氏距离找出其**近邻点**,然后建立一个**近邻连接图**,所有的近邻相连接,非近邻不连接,然后任意两点间的距离就变成了找**最短路径**的问题,然后通过10.2中的**MDS**来获取样本点在低维空间的坐标.
>近邻连接图
```handdrawn-ink
{
	"versionAtEmbed": "0.3.4",
	"filepath": "ML_Advanced/降维与度量学习/Ink/Drawing/2025.12.15 - 15.24pm.drawing",
	"width": 350,
	"aspectRatio": 1.879786006503724
}
```
在近邻连接图上计算两点间最短路径:Dijkstra 或 Floyd算法
得到任意两点距离后构建距离矩阵,然后输入给MDS,获得低维空间中的坐标

#### 2)预测
对于新样本,如何映射到低维?

将训练样本的高维坐标作为输入,低维坐标作为输出,训练一个回归学习器,预测新样本的低维坐标.(权宜之计)
#### 3)构建
1.可以指定欧氏距离最近的k个样本---->but距离很远的可能误认为是邻近
2.可以指定距离阈值c,小于c的认为是邻近点---->but可能阈值c之内没有点,断路

### 10.5.2局部线性嵌入(LLE)
>[!note]
**思想**:试图保持邻域内样本之间的线性关系

>![[Pasted image 20251215210523.png]]

## 10.6度量学习
>[!note]
>降维的目的:找到一个合适的低维空间,在这个空间中学习性能更好.
>而每个空间事实上对应了在样本属性上定义的一个距离度量,不如直接学习出一个合适的距离度量
>(metric learning)

要对距离度量进行学习,首先要定义一个可以学习的距离度量,即有**可变的参数**进行优化

### 1)推导:
基于欧氏距离:欧式距离,平方
>![[Pasted image 20251215214351.png]]

加权(即可优化的参数)
>![[Pasted image 20251215214457.png]]

这里的W已经可以进行学习,但是我们再进一步,W非对角元素为0,意味着坐标轴是正交的,认为属性间无关.考虑一般情形,将W替换为一个普通的半正定对称矩阵M(**度量矩阵**),得到**马氏距离**
>![[Pasted image 20251215214823.png]]

*怎么学?* 若我们的目标是提高分类器性能,则可以将M直接嵌入到邻近分类器的评价指标,并对此进行优化.以**近邻成分分析(NCA)**为例
##### NCA

近邻分类器做加权投票:

样本j对i的权重:
>![[Pasted image 20251215215451.png]]


与i 类别相同的j 的概率累加,就是正确的概率
那么在整个数据集上:
>![[Pasted image 20251215220215.png]]

(10.35)待入(10.37),计算错误率,再考虑M=PP<sup>T</sup>,得到优化目标
>![[Pasted image 20251215220526.png]]

可以用随机梯度下降法求解  (**how?**)

对度量矩阵M,可以引入已有的领域内知识,令xi,xj **必连**(*M*)或**勿连**(*C*),求解在**令勿连属性距离大于1约束条件下,必连的属性距离最小的度量矩阵M**
>![[Pasted image 20251215221601.png]]


度量学习学到M之后,最后是要特征值分解,得到一组正交基:......**暂时不理解怎么用**
>![[Pasted image 20251215221826.png]]
