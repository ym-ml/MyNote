# 1.子集搜索与评价
属性--->特征(feature)
有的特征和我们要解决的问题关系很大-->相关特征
有的没什么用-->无关特征
so,要选择相关特征,即**特征选择**

特征选择
	1.解决维度灾难(与降维动机接近)
	2.降低学习难度
	3.**注意**:有一些 **"冗余特征"** -->可以由别的特征推导出来,
		比如已知长方体底面长和宽,底面积就是冗余特征.<br>
		通常冗余特征没用,但是,如果要预测的是体积,冗余特征可以降低学习难度,即此时成为了一种 **"中间概念"**
		eg:
			![[Pasted image 20251217202214.png]]
			现在有测试数据6和-1
			对第一组数据:![[Pasted image 20251217202500.png]]
			对第二组加了"冗余特征"的:![[Pasted image 20251217202640.png]]
			原训练数据接近y=(x-3)<sup>2</sup> ,显然第二组结果更准确<br>
			事实上,这属于**特征工程**中的多项式特征扩展,与核技巧也有异曲同工之妙
		本章假定数据中不涉及冗余特征

## 1.1环节
**核心**:从所有特征中筛选相关特征,怎么判断相关/无关,怎么去选出来

1)子集搜索(subset search)
	产生一个候选子集,然后添加/减少
	向前/向后/双向<br>
	向前:
		d个特征,a<sub>1</sub>,...a<sub>d</sub>,第一轮对每个特征,判断,挑出来一个最相关的,放入候选子集eg:{a<sub>3</sub>};
		然后对其余每个特征,和候选子集一起,再次判断,挑出最相关的eg:{a<sub>3</sub>,a<sub>5</sub>},然后和前一次的比较,如果比前一个效果好,加入候选子集,如果不如前一次,则退出;
		不断重复.
	向后:就是候选子集从全集开始,每次去掉一个
	双向:
		试图改进向前/向后每次只能考虑到局部的缺点,检查最后加入的k个特征.*(or走k步之后检查已有的候选子集?)*<br>
		比如向前,设置一个k,每次加入新特征之后,尝试依次删掉之前的k个特征,如果效果不变差,则确定删掉*(or每走k步,对已有候选子集做一个向后搜索)*<br>
		*貌似和书上意思不太一样*
2)子集评价(subset evaluation)
	1.基于信息增益
		给定数据集D,第i类样本占比为p<sub>i</sub>,根据候选子集A中属性的取值,将D分为了V个子集,每个子集中各属性取值相同.
			eg:
			`根蒂=硬挺&&色泽=青绿    根蒂=软&&色泽=青绿`
			`根蒂=硬挺&&色泽=乌黑    根蒂=软&&色泽=乌黑`
		则属性子集A的信息增益:
		![[Pasted image 20251217210859.png]]
		![[Pasted image 20251217210914.png]]
		信息增益越大,意味着特征子集A包含的有助于分类的信息越多
	2.许多"多样性度量"稍加调整即可用于特征子集评价(见集成学习笔记)
	
事实上,决策树就可以看作是一种特征选择

常见的特征选择方法有三类:**过滤式(filter)**,**包裹式(wrapper)**,**嵌入式(embedding)**

# 2.过滤式选择
>[!note]
>先对数据集进行特征选择("过滤"),然后再训练学习器

### Relief (对二分类):
设计一个"==相关统计量=="来度量特征的重要性(一个向量,每一个分量分别对应一个初始特征,**特征子集**的重要性由子集中所有特征对应的**分量之和**来确定)    *根据相关统计量来评价搜索到的子集*

### 关键:
**确定相关统计量.--->基于距离**
给定m个样本,对每一个样本x<sub>i</sub>,在同类样本中寻找最近邻,x<sub>i,nh</sub>(猜中近邻,near-hit);再从异类样本中寻找最近邻,x<sub>i,nm</sub>(猜错近邻,near-miss),然后统计相关量对应于属性j的分量为:
>![[Pasted image 20251217214229.png]]

在属性j上,x<sub>i</sub>和猜中近邻的区别更小(更容易猜对),则这个属性对分类更有益  
*已有度量学习的意味,这个diff也可以理解为"距离"*

这里可以只采样,而不是用所有的样本计算,因此运行效率很高

### 扩展:Relief-F(多分类)
找出所有异类的猜错近邻,做一个加权平均
>![[Pasted image 20251217215059.png]]

# 3.包裹式选择
>[!note]
>将学习器的性能作为特征子集的评价准则.
>
>最终性能比过滤式好,但是训练开销很大
### LVW(Las Vegas Wrapper)
在拉斯维加斯方法的框架下(*赌徒*)
使用**随机**策略产生特征子集A,在特征子集A上做**交叉验证**,如果泛化错误率低与之前或泛化错误率相等但是A中元素个数少,则保留A.重复直到达到限制条件或收敛

在有限时间内可能给不出结果
>![[Pasted image 20251217215941.png]]

还有一种蒙特卡罗方法