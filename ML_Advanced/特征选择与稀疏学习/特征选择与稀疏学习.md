# 1.子集搜索与评价
属性--->特征(feature)
有的特征和我们要解决的问题关系很大-->相关特征
有的没什么用-->无关特征
so,要选择相关特征,即**特征选择** 

特征选择
	1.解决维度灾难(与降维动机接近)
	2.降低学习难度
	3.**注意**:有一些 **"冗余特征"**(redundant feature) -->可以由别的特征推导出来,
		比如已知长方体底面长和宽,底面积就是冗余特征.<br>
		通常冗余特征没用,但是,如果要预测的是体积,冗余特征可以降低学习难度,即此时成为了一种 **"中间概念"**
		eg:
			![[Pasted image 20251217202214.png]]
			现在有测试数据6和-1
			对第一组数据:![[Pasted image 20251217202500.png]]
			对第二组加了"冗余特征"的:![[Pasted image 20251217202640.png]]
			原训练数据接近y=(x-3)<sup>2</sup> ,显然第二组结果更准确<br>
			事实上,这属于**特征工程**中的多项式特征扩展,与核技巧也有异曲同工之妙
		[[特征工程 |特征工程--> 衍生特征]]
		本章假定数据中不涉及冗余特征 ^redundant

## 1.1环节
**核心**:从所有特征中筛选相关特征,怎么判断相关/无关,怎么去选出来

1)子集搜索(subset search)
	产生一个候选子集,然后添加/减少
	向前/向后/双向<br>
	向前:
		d个特征,a<sub>1</sub>,...a<sub>d</sub>,第一轮对每个特征,判断,挑出来一个最相关的,放入候选子集eg:{a<sub>3</sub>};
		然后对其余每个特征,和候选子集一起,再次判断,挑出最相关的eg:{a<sub>3</sub>,a<sub>5</sub>},然后和前一次的比较,如果比前一个效果好,加入候选子集,如果不如前一次,则退出;
		不断重复.
	向后:就是候选子集从全集开始,每次去掉一个
	双向:
		试图改进向前/向后每次只能考虑到局部的缺点,检查最后加入的k个特征.*(or走k步之后检查已有的候选子集?)*<br>
		比如向前,设置一个k,每次加入新特征之后,尝试依次删掉之前的k个特征,如果效果不变差,则确定删掉*(or每走k步,对已有候选子集做一个向后搜索)*<br>
		*貌似和书上意思不太一样*
2)子集评价(subset evaluation)
	1.基于信息增益
		给定数据集D,第i类样本占比为p<sub>i</sub>,根据候选子集A中属性的取值,将D分为了V个子集,每个子集中各属性取值相同.
			eg:
			`根蒂=硬挺&&色泽=青绿    根蒂=软&&色泽=青绿`
			`根蒂=硬挺&&色泽=乌黑    根蒂=软&&色泽=乌黑`
		则属性子集A的信息增益:
		![[Pasted image 20251217210859.png]]
		![[Pasted image 20251217210914.png]]
		信息增益越大,意味着特征子集A包含的有助于分类的信息越多
	2.许多"多样性度量"稍加调整即可用于特征子集评价(见集成学习笔记)
	
事实上,决策树就可以看作是一种特征选择

常见的特征选择方法有三类:**过滤式(filter)**,**包裹式(wrapper)**,**嵌入式(embedding)**

# 2.过滤式选择
>[!note]
>先对数据集进行特征选择("过滤"),然后再训练学习器

### Relief (对二分类):
设计一个"==相关统计量=="来度量特征的重要性(一个向量,每一个分量分别对应一个初始特征,**特征子集**的重要性由子集中所有特征对应的**分量之和**来确定)    *根据相关统计量来评价搜索到的子集*

### 关键:
**确定相关统计量.--->基于距离**
给定m个样本,对每一个样本x<sub>i</sub>,在同类样本中寻找最近邻,x<sub>i,nh</sub>(猜中近邻,near-hit);再从异类样本中寻找最近邻,x<sub>i,nm</sub>(猜错近邻,near-miss),然后统计相关量对应于属性j的分量为:
>![[Pasted image 20251217214229.png]]

在属性j上,x<sub>i</sub>和猜中近邻的区别更小(更容易猜对),则这个属性对分类更有益  
*已有度量学习的意味,这个diff也可以理解为"距离"*

这里可以只采样,而不是用所有的样本计算,因此运行效率很高

### 扩展:Relief-F(多分类)
找出所有异类的猜错近邻,做一个加权平均
>![[Pasted image 20251217215059.png]]

# 3.包裹式选择
>[!note]
>将学习器的性能作为特征子集的评价准则.
>
>最终性能比过滤式好,但是训练开销很大
### LVW(Las Vegas Wrapper)
在拉斯维加斯方法的框架下(*赌徒*)
使用**随机**策略产生特征子集A,在特征子集A上做**交叉验证**,如果泛化错误率低与之前或泛化错误率相等但是A中元素个数少,则保留A.重复直到达到限制条件或收敛

在有限时间内可能给不出结果
>![[Pasted image 20251217215941.png]]

还有一种蒙特卡罗方法


# 4.嵌入式选择与L1正则化
>[!note]
>过滤式和包裹式中特征选择和学习器训练有明显区别
>
>而嵌入式特征选择将**特征选择**与**学习器训练**两个过程融为一体

给定数据集D,考虑最简单的线性回归,以平方误差为损失函数.
>![[Pasted image 20251218104344.png]]

而样本特征过多,样本数过少时,容易过拟合,此时,常引入正则化项.如L2范数正则化
>![[Pasted image 20251218104511.png]]

也可以采用L1范数,此时相比L2,还有一个好处--更易于获得==稀疏(sparse)解==,即w中有更多分量为0(便可起到**特征选择**的作用)
	L1范数:所有分量的绝对值之和
	若想得到稀疏解,最自然的是L0范数,但是L0范数不连续,难以优化
>![[Pasted image 20251218105126.png]]
>*LASSO直译:最小绝对收缩选择算子*

为了便于理解为何更易于稀疏解,以有两个特征为例,则(11.6)和(11.7)都会得到两个w,w<sub>1</sub>,w<sub>2</sub>,然后如图
>![[Pasted image 20251218105459.png]]

使用L1范数,w中的一些分量更趋近于0
## 求解:近端梯度下降(PGD)
>[!note]
>PGD适合求解目标函数由**可微**和**不可微**两部分组成的问题

优化目标:(这里的x就是要求的参数w)
>![[Pasted image 20251218111338.png]]

先对前半部分可微的做普通的梯度下降
*这里通过泰勒展开近似来求解,好处:可通过 Lipschitz 常数 L 确定最大安全步长*
*不用也行*
	L-Lipschitz:
	如果一个函数满足Lipschitz条件，则存在一个常数L，使得对于所有的x和y，函数的变化量不超过L乘以x和y之间的距离。
>![[Pasted image 20251218112934.png]]
>![[Pasted image 20251218112958.png]]
这里就获得了梯度下降的步长:1/L

而将(11.10)推广到(11.8),得到每一步迭代:
>![[Pasted image 20251218113154.png]]

求解(11.12):
>![[Pasted image 20251218113339.png]]
即先对前半部分做梯度下降,得到临时变量z

>![[Pasted image 20251218113552.png]]

**(11.13)具体求解:**
	 **1. 问题拆解**
		式 (11.13) 为:$$x_{k+1} = \arg \min_{x} \frac{L}{2} \| x - z \|_2^2 + \lambda \| x \|_1.$$
		目标函数写为分量形式（设 $x, z \in \mathbb{R}^n$）:$$\frac{L}{2} \sum_{i=1}^n (x^i - z^i)^2 + \lambda \sum_{i=1}^n |x^i|$$
		由于目标函数是各分量之和，且没有交叉项（即 \(x^i x^j\ (i \neq j)\)），因此可以**对每个分量独立求解最小值**。对于第 \(i\) 个分量，问题简化为：$$\min_{x^i} \left\{ \frac{L}{2} (x^i - z^i)^2 + \lambda |x^i| \right\}$$
	**2. 简化为一元优化问题**
		令 $u = x^i, a = z^i, 常数 c = \lambda / L$，则问题等价于：$$\min_{u \in \mathbb{R}} \left\{ \frac{1}{2} (u - a)^2 + c |u| \right\}$$其中乘数 L/2 不影响极值点（可约去公因子 L)<br>
		==定义函数==：$$h(u) = \frac{1}{2} (u - a)^2 + c |u|$$因为$|u| 在 u=0$ 处不可导，需分情况讨论。
	**3. 分情况求解**
		**情况 1：\(u > 0\)**
			此时 \(|u| = u\)，有：$$h(u) = \frac{1}{2} (u - a)^2 + c u$$求导并令导数为零:$$h'(u) = (u - a) + c = 0 \quad \Rightarrow \quad u = a - c$$该解成立需满足 \(u > 0\)，即 \(a - c > 0\)，亦即 \(a > c\)
		**情况 2：\(u < 0\)**
			此时 \(|u| = -u\)，有：$$h(u) = \frac{1}{2} (u - a)^2 - c u$$求导并令导数为零：$$h'(u) = (u - a) - c = 0 \quad \Rightarrow \quad u = a + c$$该解成立需满足 \(u < 0\)，即 \(a + c < 0\)，亦即 \(a < -c\)。
		 **情况 3：\(u = 0\)**
			 由于 \(|u|\) 在 \(u=0\) 处不可导，需使用**次梯度**判断最优性。函数 h(u) 在 u=0 处的次梯度为：$$\partial h(0) = (0 - a) + c \cdot \partial |0| = -a + c \cdot [-1, 1] = [-a - c,\ -a + c]$$u=0是最优解当且仅当 $0 \in \partial h(0)$，即：$$-a - c \leq 0 \leq -a + c \quad \Leftrightarrow \quad |a| \leq c$$
	 **4. 合并结果**
		 综合三种情况：
			- 若 \(a > c\)，则最优解 \(u = a - c\)（对应 \(u > 0\)）；
			- 若 \(a < -c\)，则最优解 \(u = a + c\)（对应 \(u < 0\)）；
			- 若 $|a| \leq c$，则最优解 \(u = 0\)。
		将 $u = x^i, a = z^i, c = \lambda / L$代回，即得式 (11.14)：$$x_{k+1}^i = \begin{cases} z^i - \lambda / L, & \lambda / L < z^i; \\0, & |z^i| \leq \lambda / L; \\z^i + \lambda / L, & z^i < -\lambda / L.\end{cases}$$
	**5. 几何直观**
		该解也称为 **软阈值函数**（soft-thresholding）：
		- 当$z^i 的绝对值小于阈值 \lambda/L$ 时，系数被“压缩”为零（产生稀疏性）；
		- 否则，系数向零收缩 $\lambda/L$个单位。


## 算法:
[[嵌入式选择与L1正则化的算法]]

# 5.稀疏表示与字典学习
