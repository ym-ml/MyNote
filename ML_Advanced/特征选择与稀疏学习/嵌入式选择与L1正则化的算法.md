## 一、算法核心思想

  

**用迭代方式解决：** `最小化 f(α) + λ‖α‖₁`

- **f(α)**：可微的损失函数（如平方损失）

- **λ‖α‖₁**：不可微的 L1 正则项

  

**核心策略：** 每一步迭代中，先用梯度下降处理可微部分，再用**软阈值函数**精确处理 L1 正则。

  

## 二、具体实施步骤

  

### **第 1 步：准备工作**

1. **确定问题：** 明确你要优化的目标函数形式

   ```

   例如：LASSO回归：min 1/(2m)‖y - Xα‖² + λ‖α‖₁

   ```

2. **设定参数：**

   - 正则化强度 λ（需要调优的超参数）

   - 初始点 α₀（可设为全零向量或随机小值）

   - Lipschitz常数 L（或通过线搜索动态确定）

   - 收敛阈值 ε 和最大迭代次数 T

  

### **第 2 步：计算 Lipschitz 常数 L**

有两种方式：

1. **理论计算：** 如果 f(α) 是二次函数（如平方损失），则 L = 最大特征值(XᵀX)/m

2. **回溯线搜索（更常用）：**

   ```

   初始设 L₀ = 1，参数 η > 1（如 η=1.5）

   在每次迭代中尝试增加 L，直到满足条件：

   f(α_new) ≤ f(α_old) + ⟨∇f(α_old), α_new-α_old⟩ + (L/2)‖α_new-α_old‖²

   ```

  

### **第 3 步：主迭代循环（k=0,1,2,...）**

  

对于第 k 次迭代：

  

#### **子步骤 3.1：计算梯度**

```

计算梯度向量：∇f(αₖ)

例如对于平方损失：∇f(αₖ) = -Xᵀ(y - Xαₖ)/m

```

  

#### **子步骤 3.2：计算临时变量 z**

```

z = αₖ - (1/L) · ∇f(αₖ)

```

这是对可微部分 f(α) 做一步**普通梯度下降**。

  

#### **子步骤 3.3：应用软阈值（关键步骤）**

对 z 的**每一个分量** zⁱ 独立操作：

```

如果 zⁱ > λ/L:

    αₖ₊₁ⁱ = zⁱ - λ/L

如果 |zⁱ| ≤ λ/L:

    αₖ₊₁ⁱ = 0

如果 zⁱ < -λ/L:

    αₖ₊₁ⁱ = zⁱ + λ/L

```

  

**直观解释：**

- 当 |zⁱ| 很小时（≤ λ/L），直接将该系数设为 0 → **产生稀疏性**

- 否则，将 zⁱ 向零收缩 λ/L 个单位 → **压缩系数大小**

  

### **第 4 步：检查收敛条件**

常用收敛准则（满足任一即可停止）：

1. **相对变化小：** ‖αₖ₊₁ - αₖ‖ / max(1, ‖αₖ‖) < ε（如 ε=1e-6）

2. **函数值稳定：** |f(αₖ₊₁)-f(αₖ)| < ε

3. **达到最大迭代：** k ≥ T

  

如果未收敛，令 k = k+1，返回第 3 步。

  

## 三、实际应用示例（以 LASSO 回归为例）

  

### **输入数据：**

- 设计矩阵 X ∈ ℝ^{m×n}（m 样本，n 特征）

- 响应向量 y ∈ ℝ^{m}

- 正则化参数 λ

  

### **代码式伪算法：**

```python

def lasso_pgd(X, y, lambda_, max_iter=1000, tol=1e-6):

    # 初始化

    m, n = X.shape

    alpha = np.zeros(n) # 初始解

    L = np.linalg.norm(X.T @ X, 2) / m # Lipschitz常数

    for k in range(max_iter):

        # 1. 计算梯度

        residual = y - X @ alpha

        gradient = -X.T @ residual / m

        # 2. 计算临时变量

        z = alpha - gradient / L

        # 3. 软阈值

        alpha_new = np.zeros_like(z)

        idx_pos = z > lambda_/L

        idx_neg = z < -lambda_/L

        alpha_new[idx_pos] = z[idx_pos] - lambda_/L

        alpha_new[idx_neg] = z[idx_neg] + lambda_/L

        # |z| ≤ λ/L 的分量保持为0

        # 4. 检查收敛

        if np.linalg.norm(alpha_new - alpha) < tol:

            break

        alpha = alpha_new

    return alpha

```

  

## 四、关键要点与注意事项

  

### **1. 稀疏性的产生**

- 软阈值函数会**将小系数精确置零**，这是 LASSO 产生稀疏解的直接原因

- λ 越大，被置零的分量越多，模型越稀疏

  

### **2. 步长选择**

- 步长为 1/L，确保算法收敛

- 如果 L 估计不准，可使用**回溯线搜索**自适应调整

  

### **3. 与普通梯度下降的区别**

- 普通梯度下降无法处理 L1 正则（因为不可微）

- PGD 通过软阈值函数**显式处理**不可微部分

  

### **4. 扩展性**

- 该方法不仅适用于 LASSO，还适用于：

  - 逻辑回归 + L1 正则

  - 弹性网络（L1+L2）

  - 任何"可微损失 + L1 正则"的组合

  

## 五、实战建议

  

1. **特征标准化：** 使用前先将 X 的每一列标准化（均值为0，标准差为1），确保正则化公平对待所有特征

  

2. **λ 的选择：** 使用交叉验证选择最佳 λ

   - λ 过大：模型过于简单，欠拟合

   - λ 过小：模型接近无约束解，稀疏性弱

  

3. **加速技巧：** 可结合 Nesterov 动量加速（FISTA算法），收敛更快

  

4. **停止准则：** 实践中常用相对变化 < 1e-4 作为停止条件

  

这种 PGD 方法将复杂的 L1 正则优化问题，转化为**简单的梯度计算 + 分量独立的软阈值操作**，计算效率高，特别适合**高维数据**（特征数 n 很大）的场景。