# 1.任务与奖赏
我们考虑如何种西瓜,播种,浇水,施肥...最后种出一个瓜
若将得到好瓜作为最终奖赏, 在过程中每一步进行后,我们并不能立即获得这个奖赏,仅能得到一个反馈(eg:瓜苗看起来更健壮)
我们需多次种瓜,不断摸索,才能总结出经验.此即 **强化学习(reinforcement learning)**

![[Pasted image 20251231153411.png]]
强化学习任务通常用马尔可夫决策过程(Markov Decision Process,**MDP**)来描述:
1. 机器处于环境 $E$ 中
2. 状态空间为 $X$ ,其中每个状态$x \in X$ 是机器感知到的环境的描述.
3. 机器能采取的动作构成动作空间 $A$.
4. 若某个动作 $a \in A$ 作用在状态 $x$上,则潜在的转移函数 $P$ 将使环境从当前状态按某一概率转移到另一状态.
5. 在转移到另一状态的同时,环境会根据潜在的"奖赏"(reward)函数 $R$反馈给机器一个奖赏.
6. 综合起来,强化学习任务对应了四元组 $E=<X,A,P,R>$.

一个给西瓜浇水的简单例子:![[Pasted image 20251231154241.png]]

需注意机器与环境的界限. 
*eg:种西瓜中,环境是西瓜生长的自然世界;下棋中,环境是棋盘和对手.*

环境中状态的转移/奖赏的返回是不受机器控制的,
机器只能通过选择要执行的动作来影响环境,也只能通过观察转移后的状态和返回的奖赏来感知环境.

机器要做的是通过在环境中不断尝试而学得一个策略(policy) $\pi$ ,根据这个策略,在状态 $x$ 下就能得知要执行的动作 $a=\pi (x)$ .
策略有两种表示方法:
1. 函数 $\pi : X \mapsto A$,确定性策略
2. 概率 $\pi : X \times A \mapsto \mathbb R$,随机性策略 $\pi (x,a)$为状态 $x$下选择动作$a$ 的概率,这里必须有**概率和为1**.

策略的优劣取决于长期执行这一策略的累积奖赏.学习的目的就是找到使长期累积奖赏最大化的策略.

长期累计奖赏常用的算法: 
	1. T步累积奖赏: $\mathbb{E}\left[\frac{1}{T}\sum_{t=1}^{T}r_t\right]$
	2. $γ$ 折累积奖赏: $\mathbb{E}\left[\sum_{t=0}^{+\infty}\gamma^t r_{t+1}\right]$ 
其中,$r_t$表示第 t 步获得的奖赏值. ^longtimereward

----
对比:

| 强化学习      | 监督学习    |
| --------- | ------- |
| 状态        | 示例      |
| 动作(连续/离散) | 标记      |
| 策略        | 分类器/回归器 |
不同: 强化学习中没有监督学习中的有标记样本(即"示例-标记"对)
*换言之，没有人直接告诉机器在什么状态下应该做什么动作，只有等到最终结果揭晓，才能通过 "反思"之前的动作是否正确来进行学习*

强化学习在某种意义上可看作具有"延迟标记信息"的监督学习问题.

# 2. K-摇臂赌博机
## 2.1 探索与利用

我们不妨先仅考虑最大化单步奖赏.
*注意,即使只考虑单步,与监督学习仍有和大不同,没有数据告诉机器应当做哪个动作*

欲最大化单步奖赏,考虑两个方面:
1. 知道每个动作带来的奖赏
2. 执行奖赏最大的动作
简单的想法,每个动作都试一下就知道了.
问题在于:一般一个动作的奖赏值是来自于一个概率分布,仅通过一次尝试无法确切获得奖赏期望

单步强化学习任务对应了一个理论模型,"K-摇臂赌博机"(K-armed bandit)
	K-摇臂赌博机有 K 个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币. 但这个概率赌徒并不知道.赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币.
	![[Pasted image 20251231164507.png]]

- 仅探索法:仅为获知每个摇臂的期望奖赏.
	轮流按下各个摇臂,最终将吐出硬币的平均概率作为奖赏期望的近似
- 仅利用法: 仅为执行奖赏最大的动作
	按下目前最优的摇臂.若多个同样,随机选一个
显然,总尝试次数有限, "探索"(估计摇臂的优劣)和"利用"(选择当前最优摇臂)这两者是矛盾的.  
**"探索-利用窘境"**(Exploration-Exploitation dilemma)
显然，欲累积奖赏最大，则必须在探索与利用之间达成较好的折中.

## 2.2 $\epsilon$-贪心

>[!tldr]
>$\epsilon$-贪心法基于一个概率来对探索和利用进行折中:
>每次尝试时,以 $\epsilon$ 的概率进行探索,以 $1-\epsilon$ 的概率进行利用

令 $Q(k)$ 记录摇臂 $k$ 的平均奖赏。若摇臂 $k$ 被尝试了 $n$ 次，得到的奖赏为 $v_1, v_2, \ldots, v_n$，则平均奖赏为: $$Q(k) = \frac{1}{n} \sum_{i=1}^{n} v_i .$$
这个式子不便于计算,我们改写成可增量计算的递推公式:$$
\begin{aligned}
Q_n(k) &= \frac{1}{n} ((n-1) \times Q_{n-1}(k) + v_n)\\
&=Q_{n-1}(k)+ \frac{1}{n}(v_n-Q_{n-1}(k))
\end{aligned}\tag{16.3}$$
算法如图:![[Pasted image 20251231170359.png]]

若当前摇臂奖赏的不确定性较大,(eg:概率分布较宽),则需更多探索,设置较大的$\epsilon$ 值;否则,需要较小的$\epsilon$
当尝试次数很多时,不在需要探索,这种情况可令$\epsilon$ 的值随尝试次数的增加而减少.(eg:令$\epsilon =\frac{1}{\sqrt t}$)

## 2.3 Softmax
>[!tldr]
>基于奖赏的高低为每个摇臂设置一个选取概率
>奖赏高的摇臂选中的概率高

Softmax中摇臂概率的分配是基于Boltzmann分布:$$P(k) = \frac{e^{\frac{Q(k)}{\tau}}}{\sum_{i=1}^{K} e^{\frac{Q(i)}{\tau}}} \tag{16.4}$$
其中, $Q(i)$ 记录当前摇臂的平均奖赏; $\tau>0$称为"温度",$\tau$ 越小,则平均奖励高的摇臂被选中的概率越高.
$\tau$ 趋于0时Softmax将趋于"仅利用", 趋于无穷大时Softmax则将趋于"仅探索"

算法如图![[Pasted image 20251231171924.png]]

## 2.4 补充
1. $\epsilon$-贪心算法与 Softmax算法孰优孰劣，主要取决于具体应用.
	考虑一个简单的例子:假定2-摇臂赌博机的摇臂1 以 0.4 的概率返回奖赏 1，以0.6 的概率返回奖赏0; 摇臂 2 以 0.2 的概率返回奖赏 1， 以 0.8 的概率返回奖赏0。
	不同参数下平均奖赏如图
	>![[Pasted image 20251231195527.png]]
2. 对于离散状态空间、离散动作空间上的**多步**强化学习任务，一种直接的办法是将每个状态上动作的选择看作一个K-摇臂赌博机问题，用强化学习任务的**累积奖赏**来**代替**K-摇臂赌博机算法中的奖赏函数.<br>    当然,这种做法没有考虑马尔可夫决策过程,有很多局限

# 3.有模型学习
考虑多步强化学习任务,暂且假定任务对应的马尔可夫决策过程四元组$E=<X,A,P,R>$均为已知. 
这样的情形称为"模型已知".在已知模型的环境中学习称为"有模型学习"(model-based learning)

此时，对于任意状态 $x, x'$ 和动作 $a$，
- $P_{x\to x'}^a$ :在 $x$ 状态下执行动作 $a$ 转移到 $x'$ 状态的概率
- $R_{x\to x'}^a$ :该转移所带来的奖赏 
二者均已知.
为便于讨论，不妨假设状态空间 $X$ 和动作空间 $A$ 均为有限。

## 3.1策略评估
在模型已知时，对任意策略 $\pi$ 能估计出该策略带来的期望累积奖赏。
令
- 函数 $V^\pi(x)$ 表示从状态 $x$ 出发，使用策略 $\pi$ 所带来的累积奖赏； 
	- $V(\cdot)$ 称为“状态值函数”（state value function）
- 函数 $Q^\pi(x,a)$ 表示从状态 $x$ 出发，执行动作 $a$ 后再使用策略 $\pi$ 带来的累积奖赏。
	- $Q(\cdot)$ 称为“状态-动作值函数”（state-action value function）
这里的$V(\cdot),Q(\cdot)$分别表示指定“状态”上以及指定“状态-动作”上的==累积奖赏==。


![[强化学习#^longtimereward|长期累计奖赏]]
由累计奖赏的定义,有状态值函数:$$
\begin{cases} 
V_T^{\pi}(x) = \mathbb{E}_\pi \left[ \frac{1}{T} \sum_{t=1}^{T} r_t | x_0 = x \right], & T \text{ 步累积奖赏}; \\ 
V_{\gamma}^{\pi}(x) = \mathbb{E}_\pi \left[ \sum_{t=0}^{+\infty} \gamma^t r_{t+1} | x_0 = x \right], & \gamma \text{ 折扣累积奖赏}. 
\end{cases} \tag{16.5}$$
令 $x_0$ 表示起始状态，$a_0$ 表示起始状态上采取的第一个动作；
对于 $T$ 步累积奖赏，用下标 $t$ 表示后续执行的步数。我们有状态-动作值函数:$$
\begin{cases} 
Q_T^{\pi}(x, a) = \mathbb{E}_\pi \left[ \frac{1}{T} \sum_{t=1}^{T} r_t | x_0 = x, a_0 = a \right]; \\ 
Q_{\gamma}^{\pi}(x, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{+\infty} \gamma^t r_{t+1} | x_0 = x, a_0 = a \right]. 
\end{cases} \tag{16.6}$$
马尔可夫决策过程(MDP)具有马尔可夫性,即系统下一时刻状态只由当前时刻状态决定,
所以值函数有很简单的递归形式 *(亦称Bellman等式)*$$
\begin{aligned}
V_T^{\pi}(x) &= \mathbb{E}_\pi \left[ \frac{1}{T} \sum_{t=1}^{T} r_t | x_0 = x \right]\\
&= \mathbb{E}_\pi \left[ \frac{1}{T} r_1 + \frac{T-1}{T} \frac{1}{T-1} \sum_{t=2}^{T} r_t | x_0 = x \right]\\
&= \sum_{a \in A} \pi(x, a) \sum_{x' \in X} P_{x \rightarrow x'}^a \left( \frac{1}{T} R_{x \rightarrow x'}^a + \frac{T-1}{T} \mathbb{E}_\pi \left[ \frac{1}{T-1} \sum_{t=1}^{T-1} r_t | x_0 = x' \right] \right)\\
&= \sum_{a \in A} \pi(x, a) \sum_{x' \in X} P_{x \rightarrow x'}^a \left( \frac{1}{T} R_{x \rightarrow x'}^a + \frac{T-1}{T} V_{T-1}^{\pi}(x') \right). \end{aligned}\tag{16.7}$$
>[!note]
>1. 通过状态转移概率将 $V_{T}^{\pi}$ 和 $V_{T-1}^{\pi}$相联系
>2. 动作-状态全概率展开,即将期望展成了加权平均的形式

类似的,对 $γ$ 折扣累积奖赏,有$$V_{\gamma}^{\pi}(x) = \sum_{a \in A} \pi(x, a) \sum_{x' \in X} P_{x \rightarrow x'}^a (R_{x \rightarrow x'}^a + \gamma V_{\gamma}^{\pi}(x')). \tag{16.8}$$
需注意的是，正是由于 $P$ 和 $R$ 已知，才可以进行全概率展开。

读者可能己发现，用上面的递归等式来计算值函数，实际上就是一种动态规划算法  [failure|暂时不熟动态规划]
- 对于 $V_T^{\pi}$，可设想递归一直进行下去，直到最初的起点；
- 说人话就是，从 $V_0^{\pi}$ 出发，通过一次迭代能计算出每个状态的单步奖赏 $V_1^{\pi}$，进而得到$V_2^{\pi},V_3^{\pi}...$

算法如下
![[Pasted image 20251231203058.png]]
对于 $V_{\gamma}^{\pi}$，由于 $\gamma^t$ 在 $t$ 很大时趋于 0 ($V_{T}^{\pi}$是$\frac{1}{t}$会趋近于0), 因此也能使用类似的算法，只需将图 16.7 算法的第 3 行根据式 (16.8) 进行替换。

此外，由于算法可能会迭代很多次，因此需设置一个停止准则。常见的是设置一个阈值 $\theta$，若在执行一次迭代后值函数的改变小于 $\theta$ 则算法停止；相应的，图 16.7 算法第 4 行中的 $t = T + 1$ 需替换为==小于这个阈值==$$\max_{x \in X} |V(x) - V'(x)| < \theta .
\tag{16.9}$$
有了状态值函数 $V$，就能直接计算出状态-动作值函数
$$\begin{cases}
Q_T^{\pi}(x, a) = \sum_{x' \in X} P_{x \to x'}^a (\frac{1}{T} R_{x \to x'}^a + \frac{T-1}{T} V_{T-1}^{\pi}(x')); \\
Q_\gamma^{\pi}(x, a) = \sum_{x' \in X} P_{x \to x'}^a (R_{x \to x'}^a + \gamma V_\gamma^{\pi}(x')).
\end{cases}
\tag{16.10}$$

## 3.2 策略改进
对某个策略的累积奖赏进行评估后,若发现其非最优,我们想进行改进.
下面讨论如何改进.

一个学习任务可能有多个最优策略
理想的策略应能最大化累积奖励$$\pi^* = \arg \max_{\pi} \sum_{x \in X} V^{\pi}(x). \tag{16.11}$$
最优策略所对应的值函数 $V^*$ 称为最优值函数，即$$\forall x \in X : V^*(x) = V^{\pi^*}(x). \tag{16.12}$$
注意:==累积奖赏值最大的不一定是最优策略,还需满足约束(如果有)==
- 当策略空间无约束时式(16.12)的 $V^*$ 才是最优策略对应的值函数
	- 例如对离散状态空间和离散动作空间，策略空间是所有状态上所有动作的组合，共有 $|A|^{|X|}$ 种不同的策略。
- 若策略空间有约束，则违背约束的策略是“不合法”的，即便其值函数所取得的累积奖赏值最大，也不能作为最优值函数。

由于最优值函数的累积奖赏值已达最大，因此可对前面的 Bellman 等式(16.7)和(16.8)做一个改动，即将对动作的求和改为取最优(最大):
 >$$V_T^{\pi}(x)=\sum_{a \in A} \pi(x, a) \sum_{x' \in X} P_{x \rightarrow x'}^a \left( \frac{1}{T} R_{x \rightarrow x'}^a + \frac{T-1}{T} V_{T-1}^{\pi}(x') \right). \tag{16.7}$$
 >$$V_{\gamma}^{\pi}(x) = \sum_{a \in A} \pi(x, a) \sum_{x' \in X} P_{x \rightarrow x'}^a (R_{x \rightarrow x'}^a + \gamma V_{\gamma}^{\pi}(x')). \tag{16.8}$$
 
$$\begin{aligned}
&V_T^*(x) = \max_{a \in A} \sum_{x' \in X} P_{x \to x'}^a \left( \frac{1}{T} R_{x \to x'}^a + \frac{T-1}{T} V_T^*(x') \right)\\
&V_\gamma^*(x) = \max_{a \in A} \sum_{x' \in X} P_{x \to x'}^a \left( R_{x \to x'}^a + \gamma V_\gamma^*(x') \right).\end{aligned} \tag{16.13}$$
换言之,$$V^*(x) = \max_{a \in A} Q^{\pi^*}(x, a). \tag{16.14}$$
代入式(16.10)可得最优状态-动作值函数$$\begin{cases}
Q_T^*(x, a) = \sum_{x' \in X} P_{x \to x'}^a \left( \frac{1}{T} R_{x \to x'}^a + \frac{T-1}{T} \max_{a' \in A} Q_T^*(x', a') \right); \\
Q_\gamma^*(x, a) = \sum_{x' \in X} P_{x \to x'}^a \left( R_{x \to x'}^a + \gamma \max_{a' \in A} Q_\gamma^*(x', a') \right).
\end{cases} \tag{16.15}$$
上述关于最优值函数的等式,称为最优Bellman等式, 其唯一解是最优值函数

----
<span style="color:#6B8CBE">我们推出了目标,现在进行分析:</span>

最优 Bellman 等式揭示了非最优策略的改进方式：*将策略选择的动作改变为当前最优的动作*。

显然，这样的改变能使策略更好。
我们进行**证明**:
不妨令动作改变后对应的策略为 $\pi'$，改变动作的条件为 $Q^\pi(x, \pi'(x)) \geq V^\pi(x)$，以 $\gamma$ 折扣累积奖赏为例，由式(16.10)可计算出递推不等式
$$\begin{aligned} V^\pi(x) 
&\leq Q^\pi(x, \pi'(x))\\
&= \sum_{x' \in X} P_{x \to x'}^{\pi'(x)} (R_{x \to x'}^{\pi'(x)} + \gamma V^\pi(x'))\\
&\leq \sum_{x' \in X} P_{x \to x'}^{\pi'(x)} (R_{x \to x'}^{\pi'(x)} + \gamma Q^\pi(x', \pi'(x')))\\
&= \cdots\\
&= V^{\pi'}(x).\end{aligned}\tag{16.16}$$
即: $V^\pi(x) \le V^{\pi'}(x)$
可以得到,值函数对于策略的每一点改进都是单调递增的.因此对于当前策略 $\pi$，可放心地将其改进为$$\pi'(x) = \arg \max_{a \in A} Q^\pi(x, a), \tag{16.17}$$
直到 $\pi'$ 与 $\pi$ 一致、不再发生变化，此时就满足了最优 Bellman 等式，即找到了最优策略。

## 3.3策略迭代与值迭代
