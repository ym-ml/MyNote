# 1.未标记的样本
## question
我们有训练样本集$D_l=\{(x_1,y_1),(x_2,y_2),...,(x_l,y_l)\}$,这$l$个样本有标记,
还有$D_u=\{x_{l+1},x_{l+2},...,x_{l+u}\},l<<u$,这u个样本没有标记.
$l$太少且不想浪费这$u$个数据
## how
### 1)"主动学习"
最简单的,找'专家'把这u个数据标记了,不过成本太高
改进:**主动学习(active learning)**
用$D_l$个样本训练好模型,然后挑u中对训练最有帮助的,找'专家'标记,然后重新训练.不断重复.    可以使用尽量少的查询(query)
	eg:
	基于$D_l$训练一个SVM,挑选距离分类超平面最近的未标记的样本来进行查询
### 2) 半监督学习
可以不依赖外界而利用未标记样本吗?  Yes

事实上,未标记样本随未直接包含标记信息,但其所包含的关于数据分布的信息对建立模型大有裨益
	eg:
	![[Pasted image 20251223185646.png]]

**半监督学习(semi-supervised learning)**:让学习器不依赖外界交互,自动地利用未标记样本来提升学习性能.

将未标记样本所揭示的数据分布信息&类别标记相联系

#### 如何联系?
1.**聚类假设(cluster assumption)**
	最常见,假设数据存在簇结构,同一个簇的样本属于同一个类别.如上图.
	待测样本和带标记样本被未标记样本"撮合"
2.**流形假设(manifold assumption)**
	假设数据分布在一个流形结构上,邻近的样本拥有相似的输出值.
	"邻近"常用相似程度来刻画 
	[[降维与度量学习#^e28594|降维与度量学习-流形学习]]  <br>
	流形可视为聚类的推广,对输出没有限制.聚类考虑到一般是分类问题
	
	^7064ce
二者本质:相似的样本拥有相似的输出.

#### 半监督学习分类:
半监督学习可以进一步划分为纯(pure)半监督学习和直推学习(transductive learning)

纯半监督:
	'开放世界'假设,希望模型能适用于所有未观察到的数据
直推学习:
	"封闭世界",仅对无标记样本进行预测
纯半监督学习和直推学习常合称为半监督学习
![[Pasted image 20251223193023.png]]

>[!todo]
>思考
>如果用$l$个样本训练好之后,对u预测.做很多个这样的模型,用预测之后的u+l个再训练,然后做一个集成学习,有没有用?
>细节在思考一下,然后以后试一下


# 2.生成式方法(generative methods)

>[!tldr]
>直接基于**生成式模型**的方法,假设所有数据都由一个潜在模型生成,未标记数据的标记看作模型的缺失参数.

这个假设使我们能通过潜在模型的参数将未标记数据与学习目标联系起来,缺失参数(未标记数据的标记)可以通过EM算法进行极大似然估计求解

此类方法的主要区别在于生成式模型的假设, 要恰当

>[!note]
>贝叶斯分类器章节有学到,给定 $x$ ,可通过直接建模$P(c|x)$来预测$c$,即**判别式**
>也可先对联合概率分布$P(x,c)$建模,然后再由此获得$P(c|x)$,即**生成式**

## 基于高斯混合模型的假设:

给定样本$x$,其真实类别标记为$y \in \mathcal{Y}$,其中$\mathcal{Y}=\{1,2,...,N\}$为所有可能的类别.
假设样本由高斯混合模型生成,**且每个类别对应一个高斯混合成分.**
即
>![[Pasted image 20251223213743.png]]

>![[Pasted image 20251223213852.png]]$p(\Theta =i|\boldsymbol{x})$为样本$\boldsymbol{x}$由第$i$高斯混合成分生成的后验概率.

由于假设每个类别对应一个高斯混合成分(即第$i$个类别对应第$i$个高斯混合成分),因此
	$p(y=j|\Theta=i,\boldsymbol{x})$仅与$\boldsymbol{x}$所属的高斯混合成分$\Theta$有关,可用$p(y=j|\Theta=i)$代替
	$p(y=j|\Theta=i)=1$当且仅当$i=j$,否则$p(y=j|\Theta=i)=0$
(13.2)可化简为:$f(x)=\arg\max_i p(\Theta=i|x)$

且由式(13.2)可以看到, $p(\Theta=i|x)$不涉及样本标记,因此可以利用未标记数据增加其准确度



给定有标记数据集$D_l=\{(x_1,y_1),(x_2,y_2),...,(x_l,y_l)\}$,未标记数据集$D_u=\{x_{l+1},x_{l+2},...,x_{l+u}\},l<<u,l+u=m$,假设所有样本独立同分布,且都是由同一个高斯混合模型生成的.用极大似然法来估计高斯混合模型的参数$\{(\alpha_i,\mu_i,\Sigma_i)|1 \le i \le N\},D_l \cup D_u$的对数似然是:

>[!note]
>似然:
>给定参数 $\theta$ 时观测到数据集 $D$ 的概率
>独立同分布时就是每个样本出现概率的乘积

这里似然由两项组成:有标记数据 &未标记数据

对于有标记数据 $(x_j, y_j)$：
我们知道它来自类别 $y_j$，对应高斯成分 $\Theta = y_j$
该样本的概率：$\alpha_{y_j} \cdot p(x_j | \mu_{y_j}, \Sigma_{y_j})$

对于未标记数据 $x_j$:
不知道来自哪个成分，需要对所有成分求和
该样本的概率：$\sum_{i=1}^{N} \alpha_i \cdot p(x_j | \mu_i, \Sigma_i)$

可得:$$\begin{aligned}

LL(D_l \cup D_u) &= \sum_{(x_j, y_j) \in D_l} \ln\left( \alpha_{y_j} \cdot p(x_j | \mu_{y_j}, \Sigma_{y_j}) \right) \\

&\quad + \sum_{x_j \in D_u} \ln\left( \sum_{i=1}^{N} \alpha_i \cdot p(x_j | \mu_i, \Sigma_i) \right)

\end{aligned}$$
为了统一形式,给第一项加上了$\Sigma$,乘了个$p(y=j|\Theta=i,\boldsymbol{x})$,对$i \not =j$的,为0,所以结果不变.
得到(13.4)
>![[Pasted image 20251223220022.png]]

然后,EM求解参数
>![[Pasted image 20251223221958.png]]
>可通过有标记数据对模型参数进行初始化

这里可以换成朴素贝叶斯模型or混合专家模型等其他生成式模型推导相应的生成式半监督方法.
[[生成式方法的一般思路&推广]]

此类方法简单，易于实现，在有标记数据极少的情形下往往比其他方法性能更好.
然而此类方法有一个关键:模型假设必须准确，即假设的生成式模型必须与真实数据分布吻合,否则利用未标记数据反倒会降低泛化性能

# 3.半监督SVM
