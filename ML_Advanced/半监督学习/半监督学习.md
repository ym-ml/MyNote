# 1.未标记的样本
## question
我们有训练样本集$D_l=\{(x_1,y_1),(x_2,y_2),...,(x_l,y_l)\}$,这$l$个样本有标记,
还有$D_u=\{x_{l+1},x_{l+2},...,x_{l+u}\},l<<u$,这u个样本没有标记.
$l$太少且不想浪费这$u$个数据
## how
### 1)"主动学习"
最简单的,找'专家'把这u个数据标记了,不过成本太高
改进:**主动学习(active learning)**
用$D_l$个样本训练好模型,然后挑u中对训练最有帮助的,找'专家'标记,然后重新训练.不断重复.    可以使用尽量少的查询(query)
	eg:
	基于$D_l$训练一个SVM,挑选距离分类超平面最近的未标记的样本来进行查询
### 2) 半监督学习
可以不依赖外界而利用未标记样本吗?  Yes

事实上,未标记样本随未直接包含标记信息,但其所包含的关于数据分布的信息对建立模型大有裨益
	eg:
	![[Pasted image 20251223185646.png]]

**半监督学习(semi-supervised learning)**:让学习器不依赖外界交互,自动地利用未标记样本来提升学习性能.

将未标记样本所揭示的数据分布信息&类别标记相联系

#### 如何联系?
1.**聚类假设(cluster assumption)**
	最常见,假设数据存在簇结构,同一个簇的样本属于同一个类别.如上图.
	待测样本和带标记样本被未标记样本"撮合"
2.**流形假设(manifold assumption)**
	假设数据分布在一个流形结构上,邻近的样本拥有相似的输出值.
	"邻近"常用相似程度来刻画 
	[[降维与度量学习#^e28594|降维与度量学习-流形学习]]  <br>
	流形可视为聚类的推广,对输出没有限制.聚类考虑到一般是分类问题
	
	^7064ce
二者本质:相似的样本拥有相似的输出.

#### 半监督学习分类:
半监督学习可以进一步划分为纯(pure)半监督学习和直推学习(transductive learning)

纯半监督:
	'开放世界'假设,希望模型能适用于所有未观察到的数据
直推学习:
	"封闭世界",仅对无标记样本进行预测
纯半监督学习和直推学习常合称为半监督学习
![[Pasted image 20251223193023.png]]

>[!todo]
>思考
>如果用$l$个样本训练好之后,对u预测.做很多个这样的模型,用预测之后的u+l个再训练,然后做一个集成学习,有没有用?
>细节在思考一下,然后以后试一下


# 2.生成式方法(generative methods)

>[!tldr]
>直接基于**生成式模型**的方法,假设所有数据都由一个潜在模型生成,未标记数据的标记看作模型的缺失参数.

这个假设使我们能通过潜在模型的参数将未标记数据与学习目标联系起来,缺失参数(未标记数据的标记)可以通过EM算法进行极大似然估计求解

此类方法的主要区别在于生成式模型的假设, 要恰当

>[!note]
>贝叶斯分类器章节有学到,给定 $x$ ,可通过直接建模$P(c|x)$来预测$c$,即**判别式**
>也可先对联合概率分布$P(x,c)$建模,然后再由此获得$P(c|x)$,即**生成式**

## 基于高斯混合模型的假设:

给定样本$x$,其真实类别标记为$y \in \mathcal{Y}$,其中$\mathcal{Y}=\{1,2,...,N\}$为所有可能的类别.
假设样本由高斯混合模型生成,**且每个类别对应一个高斯混合成分.**
即
>![[Pasted image 20251223213743.png]]

>![[Pasted image 20251223213852.png]]$p(\Theta =i|\boldsymbol{x})$为样本$\boldsymbol{x}$由第$i$高斯混合成分生成的后验概率.

由于假设每个类别对应一个高斯混合成分(即第$i$个类别对应第$i$个高斯混合成分),因此
	$p(y=j|\Theta=i,\boldsymbol{x})$仅与$\boldsymbol{x}$所属的高斯混合成分$\Theta$有关,可用$p(y=j|\Theta=i)$代替
	$p(y=j|\Theta=i)=1$当且仅当$i=j$,否则$p(y=j|\Theta=i)=0$
(13.2)可化简为:$f(x)=\arg\max_i p(\Theta=i|x)$

且由式(13.2)可以看到, $p(\Theta=i|x)$不涉及样本标记,因此可以利用未标记数据增加其准确度



给定有标记数据集$D_l=\{(x_1,y_1),(x_2,y_2),...,(x_l,y_l)\}$,未标记数据集$D_u=\{x_{l+1},x_{l+2},...,x_{l+u}\},l<<u,l+u=m$,假设所有样本独立同分布,且都是由同一个高斯混合模型生成的.用极大似然法来估计高斯混合模型的参数$\{(\alpha_i,\mu_i,\Sigma_i)|1 \le i \le N\},D_l \cup D_u$的对数似然是:

>[!note]
>似然:
>给定参数 $\theta$ 时观测到数据集 $D$ 的概率
>独立同分布时就是每个样本出现概率的乘积

这里似然由两项组成:有标记数据 &未标记数据

对于有标记数据 $(x_j, y_j)$：
我们知道它来自类别 $y_j$，对应高斯成分 $\Theta = y_j$
该样本的概率：$\alpha_{y_j} \cdot p(x_j | \mu_{y_j}, \Sigma_{y_j})$

对于未标记数据 $x_j$:
不知道来自哪个成分，需要对所有成分求和
该样本的概率：$\sum_{i=1}^{N} \alpha_i \cdot p(x_j | \mu_i, \Sigma_i)$

可得:$$\begin{aligned}

LL(D_l \cup D_u) &= \sum_{(x_j, y_j) \in D_l} \ln\left( \alpha_{y_j} \cdot p(x_j | \mu_{y_j}, \Sigma_{y_j}) \right) \\

&\quad + \sum_{x_j \in D_u} \ln\left( \sum_{i=1}^{N} \alpha_i \cdot p(x_j | \mu_i, \Sigma_i) \right)

\end{aligned}$$
为了统一形式,给第一项加上了$\Sigma$,乘了个$p(y=j|\Theta=i,\boldsymbol{x})$,对$i \not =j$的,为0,所以结果不变.
得到(13.4)
>![[Pasted image 20251223220022.png]]

然后,EM求解参数
>![[Pasted image 20251223221958.png]]
>可通过有标记数据对模型参数进行初始化

这里可以换成朴素贝叶斯模型or混合专家模型等其他生成式模型推导相应的生成式半监督方法.
[[生成式方法的一般思路&推广]]

此类方法简单，易于实现，在有标记数据极少的情形下往往比其他方法性能更好.
然而此类方法有一个关键==:模型假设必须准确==，即假设的生成式模型必须与真实数据分布吻合,否则利用未标记数据反倒会降低泛化性能
(so,依赖专业知识)
## Think
### 1)为什么是生成式,判别式不行?
生成式是对概率分布建模,而推导可得到后验概率$p(\Theta =i |\boldsymbol x)$与标记无关
(可参考上面的"生成式方法的一般思路&推广")
### 2)生成式是在做什么?
==利用未标记数据提高模型的准确度== 所以说前面提到在用 $D_l$ 训练好之后作为初始化,再用 $D_l \cup D_u$ ,相当于修正

#### 对朴素贝叶斯假设
对基于**朴素贝叶斯**的生成式方法:提高参数精度

eg:垃圾邮件分类,有1000个带标记的邮件和10000个未标记的.
	就有点像用这1000个训练好之后,对这10000个进行预测,然后把这11000个总体再次进行训练
#### 对**高斯混合**假设
高斯混合是干什么的?--==**聚类**==
so,为什么假设一个类别对应一个分布?--**一个簇**

所以说这里的类别并不一定说二分类就只有两个类别.(*实际上，这就是**混合专家模型***)



# 3.半监督SVM

半监督支持向量机(Semi-Supervised Support Vector Machine, S3VM)

不考虑未标记样本时,支持向量机试图找到最大间隔来划分超平面. 
考虑未标记样本后,S3VM试图找找能将两类有标记样本分开,且穿过数据低密度区域的划分超平面.

基本假设:"低密度分隔".显然,这是聚类假设在考虑了线性超平面划分后的推广.
![[Pasted image 20251224145734.png]]


>[!attention]
>可能会因为数据低密度区有多个,穿过了不好的那个,导致使用未标记样本后泛化性能反而降低
>
>S4VM可以一定程度上缓解这个问题.当然,模型复杂了肯定要么开销大,要么容易过拟合

## TSVM
### 思路
>[!tldr]
针对二分类
对未标记样本进行各种可能的标记指派, 在所有指派结果中试图寻找一个在所有样本上间隔最大的划分超平面

给定有标记数据集$D_l=\{(x_1,y_1),(x_2,y_2),...,(x_l,y_l)\}$,未标记数据集$D_u=\{x_{l+1},x_{l+2},...,x_{l+u}\},l<<u,l+u=m$.TSVM的学习目标是为$D_u$中的样本给出预测标记$\hat y=(\hat y_{l+1},\hat y_{l+2},...,\hat y_{l+u}),\hat y\in\{-1,1\}$,使得:
$$\min_{w,b,\hat{y},\xi} \frac{1}{2} \| w \|_2^2 + C_l \sum_{i=1}^l \xi_i + C_u \sum_{i=l+1}^m \xi_i \tag{13.9}$$ s.t.  $$y_i (w^T x_i + b) \geq 1 - \xi_i, \quad i = 1, 2, \ldots, l,$$$$ \hat y_i (w^T x_i + b) \geq 1 - \xi_i, \quad i = l + 1, l + 2, \ldots, m, $$
$$ \xi_i \geq 0, \quad i = 1, 2, \ldots, m, $$

其中，$(w, b)$ 确定了一个划分超平面；$\xi$ 为松弛向量，$\xi_i (i = 1, 2, \ldots, l)$ 对应于有标记样本，$\xi_i (i = l + 1, l + 2, \ldots, m)$ 对应于未标记样本；
$C_l$ 与 $C_u$ 是由用户指定的用于平衡模型复杂度、有标记样本与未标记样本重要程度的折中参数。
$\hat y$ 即为待指派的标记

[[SVM /回忆 /待补笔记]]
	这是软间隔SVM中.
	$\xi$ 为松弛变量.  
	迟来的理解:实际上松弛变量代表了一个损失函数,"合页损失".可证明:$\xi_i =\max (0, 1-y_i(\boldsymbol w^T x_i+b))$.
		![[Pasted image 20251224152255.png]]

### 算法
穷举所有标记,不现实.
so,TSVM采用局部搜索迭代.
1. $D_l$ 训练;
2. 对$D_u$ 预测($D_u$带上了'伪标记');
3. 用$D_{l+u}$ 正常训练SVM(<span style="color:red">此时 D_u 中会有较多预测错误的,所以参数C_u要小</span>);
4. 找出两个异类且最可能错的预测,交换二者的标记;
5. 再次训练(<span style="color:red">此时可适当增大C_u. eg: *2</span>);
6. 重复执行4,5两步,直到$C_u=C_l$.

最终对未标记样本的预测结果就是指派的标记; 同时,可以对新样本进行预测
>![[Pasted image 20251224154339.png]]

半监督SVM的目标函数非凸，有不少工作致力于减轻非凸性造成的不利影响;
第4步计算开销很大,很多人在寻求高效求解的方法(eg:LDS,meanS3VM).
### 类别不平衡
在对未标记样本进行指派时,可能遇到类别不平衡问题.
[[回忆/类别不平衡/待补]]

将$C_u$分为$C_{u^+},C_{u^-}$两项,即正负例分开,且令$C_{u^+}=\frac{u^-}{u^+}C_{u^-}$ 
<移动阈值>的思想
很简单的推导:$\frac{u^-}{u^+ +u^-}$与$\frac{C_{u^-}}{C_{u^+} +C_{u^-}}$成反比.负类多了,那么$C_{u^-}$就应该小,减小每一个负类起到的作用.
所以$$\frac{u^-}{u^+ +u^-}=\frac{C_{u^+}}{C_{u^+} +C_{u^-}}$$
交叉相乘,化简,即可.

# 4.图半监督学习

>[!tldr]
>将数据集($D_l \cup D_u$)映射成图,有标记的染过色,未标记的没有.半监督学习就对应"颜色"在图上扩散的过程.
>
>图对应了矩阵,所以可以基于矩阵运算来推导分析

给定一个数据集，我们可将其映射为一个图，数据集中每个样本对应于图中一个结点. 
若两个样本之间的相似度很高(或相关性很强)，则对应的结点之间存在一条边, 边的"强度" (strength)正比于样本之间的相似度(或相关性). 
我们可将有标记样本所对应的结点想象为染过色，而未标记样本所对应的结点尚未染色. 于是半监督学习就对应于"颜色"在图上扩散或传播的过程.

>[!failure]
<看不懂...>

高斯距离图(本节介绍的),k近邻图,$\epsilon$近邻图都很常用 [[降维与度量学习#^k-epsilon|什么是图&k/epsilon近邻图]]
# 5.基于分歧的方法

disagreement-based method
分歧,disagreement/diversity
>[!tldr]
>使用多学习器
>代表:协同训练

协同训练(co-training),起初是为多视图数据设计的

## 5.1多视图数据
在现实应用中,一个数据对象往往同时拥有多个"属性集"(attribute set),每个属性集就构成了一个"视图"(view)
eg:
	对电影来说,有声音相关的各个属性构成一个属性集,内容相关的各个属性构成一个属性集. 这两个属性集就是两个视图.
用 $(<\boldsymbol x^1,\boldsymbol x^2>,y)$来表示多视图数据.对上面的例子,$\boldsymbol x^1$表示声音视图,$\boldsymbol x^2$表示内容视图,$y$ 表示电影类别.

多视图数据具有"相容性":
	基于$\boldsymbol x^1$预测得到的标记空间 $\mathcal Y_1$ 和基于 $\boldsymbol x^2$ 预测得到的标记空间$\mathcal Y_2$ 与样例的真实标记空间 $\mathcal Y$ 应该是一样的
基于"相容性"可得"互补性":
	若$\boldsymbol x^1$与$\boldsymbol x^2$预测结果一样,那我们就有更大的把握说这个预测是对的.


## 5.2协同训练

协同训练正是很好地利用了多视图的"相容互补性".
假设数据拥有两个**充分**(sufficient )且**条件独立**视图，"充分"是指每个视图都包含足以产生最优学习器的信息. "条件独立"则是指在给定类别标记条件下两个视图独立.

### 算法:
在此情形下，可用一个简单的办法来利用未标记数据:
1. 从$D_u$中随机初始化s个未标记的数据作为缓冲池(*若每一轮都考察所有的未标记样本,开销太大*); 
2. 在视图$\boldsymbol x^1$上训练学习器$h_1$,在视图$\boldsymbol x^2$上训练学习器$h_2$.
3. 用$h_1$在缓冲池中挑把握最大的p个正例和n个负例.$h_2$也同理;
4. 将这$2(p+n)$个样例加入到$D_l$中,缓冲池再随机补上$2(p+n)$个未标记的数据;
5. 重复执行2--4,直到$h_1,h_2$不再变化.
>![[Pasted image 20251224163609.png]]

### 理论研究
1. 理论证明显示出，若两个视图充分且条件独立，则可利用未标记样本通过协同训练将弱分类器的泛化性能提升到任意高.

   不过，视图的条件独立性在现实任务中通常很难满足，因此性能提升幅度不会那么大.
	eg:电影画面与声音显然不会是条件独立的

2. 但研究表明，即使在更弱的条件下, 协同训练仍可有效地提升弱分类器的性能

3. 后续研究表明,此类算法无需数据数据拥有多视图,仅需弱学习器之间具有显著的"分歧", 即可通过相互之间提供伪标记样本来提升泛化性能.

此类方法简单有效.不过设计"分歧"时需要一些巧思.

# 6.半监督聚类
(semi-supervised clustering)

>[!tldr]
>现实中我们常常能获得一些额外的信息,利用他们"监督"聚类可以获得更好的结果.

获得的监督信息可分为两类:
1. 有必连和勿连约束.
2. 有些样本有簇标记.(不是类别标记)

## 6.1约束k均值
处理必连和勿连约束
必连和勿连约束可联系前文:[[降维与度量学习#^must-cannot]]
该算法是k均值算法的扩展,k均值算法联系前文:[[聚类/k均值/待补]]

给定样本集$D=\{x_1,x_2,...,x_m\}$以及必连集合$\mathcal M$,勿连集合$\mathcal C$.
必连集合中的样本必须在一个簇,勿连集合中的样本必须不在一个簇.

### 算法

与k均值算法相比,在一个样本加入到距离最近的簇之前多了一步判断,如果不符合约束则换一个次近的簇尝试.
在算法中,通过对集合$\mathcal K=\{1,2,...,k\}$的变动来实现.

>![[Pasted image 20251224170217.png]]

注:此算法中不包含判断是否符合约束的具体实现.

## 6.2 约束种子k均值
第二类,少量样本有簇标记.这样的监督信息利用起来很容易.

利用已有标记的样本产生聚类中心,并初始化聚类簇.
在优化时不改变原隶属关系即可.

>![[Pasted image 20251224170933.png]]

