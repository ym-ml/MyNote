机器学习:从**已观察到的数据**估计/推测感兴趣的**未知变量.**

概率模型为此提供了一种描述框架: 学习任务-->去计算变量的概率分布
在概率中,利用已知变量推测未知变量的分布称为"推断"(inference)

假定所关心的变量集合为$Y$,可观测变量集合为$O$,其他变量集合为$R$.<br>
生成式: 考虑联合概率分布$P(Y,R,O)$.
判别式:考虑条件分布$P(Y,R|O)$.<br>
二者最后都要推断条件概率分布$P(Y|O)$.
显然,要处理掉$R$.
1. 有概率求和规则 $P(O)=\Sigma_{Y,R} P(Y,R,O)$ ,但项数太多.即使每个变量仅两个取值,那么复杂度为$O(2^{|Y|+|R|})$.
2. 属性间往往有复杂联系.

因此我们使用概率图模型

----
**概率图模型(probabilistic graphical model)**:一类用图来表达变量相关关系的概率模型.
以图为表示工具,常用一个结点表示一个/一组随机变量,结点之间的边表示变量间的概率相关关系.即"变量关系图"

根据边的性质不同,大致可分为两类
- 使用有向无环图,称为有向图模型/贝叶斯网(Bayesian network)
- 使用无向图,称为无向图模型/马尔可夫网(Markov network)

# 1. 隐马尔可夫网(HMM)
结构最简单的动态贝叶斯网.有向图.
主要用于时序数据(语音识别,自然语言处理)

![[Pasted image 20251225132343.png]]
变量分为两组:
 - $\{y_1,y_2,...,y_n\}$,其中$y_i \in \mathcal Y$表示第 $i$ 时刻的系统状态.通常假定状态变量是隐藏的,不可观测,因此又叫**隐变量**(hidden variable).
 - **观测变量**$\{x_1,x_2,...,x_n\}$,其中$x_i\in X$表示第 $i$ 时刻的观测值.

隐马尔可夫模型中,系统通常在多个状态$\{s_1,s_2,...,s_N\}$之间变化,所以
 $y_i$的取值范围$\mathcal Y$(称为状态空间)通常是有$N$个取值的离散空间;
 观测变量$x_i$可以是离散型也可以是连续型，为便于讨论, 我们仅考虑离散型观测变量，并假定其取值范围$\mathcal X为\{o_1,o_2,...,o_M\}$.

上图是一个 **"马尔科夫链"** ,箭头表示依赖关系,即:系统下一时刻的状态仅由当前状态决定，不依赖于以往的任何状态
基于此,可得所有变量的联合概率分布:$$P(x_1, y_1, \ldots, x_n, y_n) = P(y_1)P(x_1 | y_1) \prod_{i=2}^n P(y_i | y_{i-1})P(x_i | y_i)$$
$从y_2开始,y_i由y_{i-1}决定,决定了x_i$.

这是结构,除了结构,想确定一个隐马尔可夫模型还需要三组参数.
1. 状态转移概率:模型在各个状态间转移的概率,用矩阵$A = [a_{ij}]_{N \times N}$表示
    其中:$a_{ij} = P(y_{t+1} = s_j | y_t = s_i), \quad 1 \leq i, j \leq N$ 
    表示当前为状态$s_i$时下一时刻变为$s_j$的概率.
2. 输出观测概率:根据当前状态获得各个观测值的概率.用矩阵$B = [b_{ij}]_{N \times M}$表示.
    其中:$b_{ij} = P(x_t = o_j | y_t = s_i), \quad 1 \leq i \leq N, \quad 1 \leq j \leq M$
	    $\mathcal X为\{o_1,o_2,...,o_M\},|\mathcal Y|=N$所以是$N\times M$.
3. 初始状态概率:在模型初始时刻各状态出现的概率.通常记为$\pi = (\pi_1, \pi_2, \ldots, \pi_N)$.
	其中$\pi_i = P(y_1 = s_i), \quad 1 \leq i \leq N$.

通过指定状态空间$\mathcal Y$,观测空间$\mathcal X$和上述三组参数(记为$\lambda =[A,B,\pi]$)即可确定一个隐马尔可夫模型.通常用 $\lambda$ 指代这个模型

它按如下过程产生观测序列$\{x_1,x_2,...,x_n\}$: 
	(1) 设置 t = 1，并根据初始状态概率 $\pi$ 选择初始状态$y_1$; 
	(2) 根据状态 $y_t$ 和输出观测概率 $\boldsymbol B$ 选择观测变量取值$x_t$;
	(3) 根据状态 $y_t$ 和状态转移矩阵 $\boldsymbol A$ 转移模型状态，即确定$y_{t+1}$;
	(4) 若 t < n， 设置 $t = t + 1$，并转到第 (2) 步，否则停止.
很好理解

实际应用中,要解决三个问题:
1. 如何评估$\lambda$ 和$\boldsymbol x$的匹配程度? 即给定$\lambda$,如何计算产生观测序列的概率$P(\boldsymbol x|\lambda)$     *>>想根据$\{x_1,x_2,...,x_{n-1}\}$推测$x_n$*
2. 如何由$\lambda$和$\boldsymbol x$推测出隐藏的模型状态?    *>>语音识别,语音对应的文字就是隐变量$y$.*
3. 如何训练$\lambda$使其更好地描述观测数据?    *>>总不能人工设置参数吧*

所幸,基于上面式子的条件独立性,这些问题都可以高效求解.
[具体怎么解书上没说..]

# 2.马尔可夫随机场(MRF)

## 2.1定义
典型马尔可夫网,无向图
 结点:一个/一组变量
 边:两个变量之间的依赖关系
 势函数(potential functions)/因子:用于定义概率分布函数

![[Pasted image 20251225142001.png]]
如图,一个马尔可夫随机场

对图中结点集的一个子集,若任意两结点之间都有边连接,称该子集为一个团(clique).    若这个团中不能再加入结点(即这个团不能被更大的团包含),称这个团为极大团 ^clique

 图中$\{x_1,x_2\},\{x_1,x_3\},\{x_5,x_6\}...$都是一个团,而除了$\{x_2,x_5\},\{x_2,x_6\},\{x_5,x_6\}$,其余都是极大团.  

## 2.2 联合概率分布
>[!tldr]
>多个变量之间的联合概率分布能**基于团分解**为多个因子的乘积,  每个因子仅与一个团相关.

对于 $n$ 个变量$\boldsymbol x = \{x_1, x_2, \ldots, x_n\}$所有团构成的集合为 $\mathcal C$与团$Q\in \mathcal C$ 对应的变量集合记为$\boldsymbol x_Q$ ， 则联合概率$P(x)$ 定义为

$$P(x) = \frac{1}{Z} \prod_{Q \in C} \psi_Q(x_Q)$$
$Z = \sum_x \prod_{Q \in C} \psi_Q(x_Q)$为规范化因子,保证和为1是概率.$\psi_Q$为势函数.
对$\boldsymbol x_Q$ ,eg:上图中$x_1,x_2$构成了一个团$Q_{12}$,那么$\boldsymbol x_{Q_{12}}=\{x_1,x_2\}$. ^potentialf

显然,若变量数太多,团的数量会很多.而如果不是极大团$Q^*$,必被极大团包含,所以变量间的关系可用$\psi_{Q^*}$代替它的"子团"里的$\psi_{Q_{25}},\psi_{Q_{26}},\psi_{Q_{56}}$.
则基于极大团定义联合概率分布$P(x)$:$$P(x) = \frac{1}{Z^*} \prod_{Q \in C^*} \psi_Q(x_Q)$$
其中$Z^* = \sum_x \prod_{Q \in C^*} \psi_Q(x_Q)$.


对上图$\boldsymbol x = \{x_1, x_2, \ldots, x_6\}$:$$P(\boldsymbol x) = \frac{1}{Z} \psi_{12}(x_1, x_2) \psi_{13}(x_1, x_3) \psi_{24}(x_2, x_4) \psi_{35}(x_3, x_5) \psi_{256}(x_2, x_5, x_6)$$
## 2.3 条件独立性

[贝叶斯网|结构|待补]

如何得到条件独立性?

同样借助"分离"的概念
若从结点集A中的结点到B中的结点都必须经过结点集C 中的结点，则称结点集A和B被结点集C分离， C称为"分离集" (separating set).
![[Pasted image 20251225144952.png]]
对随机马尔可夫场,有:

**"全局马尔可夫性"** (global Markov property): 给定两个变量子集的分 离集，则这两个变量子集条件独立.

即$\boldsymbol x_A \perp \boldsymbol x_B |\boldsymbol x_C$.
[[全局马尔可夫性的简单验证.png]]

由全局马尔可夫性可以推出两个有用的结论:
![[Pasted image 20251225150233.png]]
1. 局部马尔可夫性:给定某变量的邻接变量,则该变量独立于其他变量.
   $x_1邻接x_4,x_2,则x_1与x_3,x_5独立$    $x_1\perp \{x_3,x_5\}|\{x_4,x_2\}$.
2. 成对马尔可夫性:给定所有其他变量，两个非邻接变量条件独立.
   $x_1\perp x_5|\{x_2,x_3,x_4\}$.

## 2.4 势函数
$\psi_Q(x_Q)$定量刻画变量集$\boldsymbol x_Q$中变量之间的相关关系.
非负且在所偏好的变量取值上有较大函数值
eg: $$\psi_{AC}(x_{A}, x_{C}) = 
\begin{cases} 
1.5, & \text{if } x_{A} = x_{C}; \\
0.1, & \text{otherwise},
\end{cases}$$
指数函数常用于定义$\psi_Q$: $\psi_{Q}(\boldsymbol x_{Q}) = e^{-H_{Q}(\boldsymbol x_{Q})}$
 其中${-H_{Q}(\boldsymbol x_{Q})}$是一个定义在$\boldsymbol x_Q$上的实值函数.
 常见:$$H_{Q}(x_{Q}) = \sum_{u,v \in Q, u \neq v} \alpha_{uv}x_{u}x_{v} + \sum_{v \in Q} \beta_{v}x_{v}$$
  其中$\alpha_{uv},\beta_{v}$是参数.第一项是考虑了每一对结点,第二项考虑单结点.
# 3.条件随机场(CRF)
Conditional Random Field
**判别式**无向图模型
可看作给定观测值的马尔可夫随机场,也可看做对率回归的扩展.[对率回归|待补]

>[!tldr]
>条件随机场试图对 多个变量 在给定观测值后的 条件概率建模

令$\mathbf x=\{x_1,x_2,...,x_n\}$为观测序列,$\mathbf y=\{y_1,y_2,...,y_n\}$为对应的标记序列.则条件随机场的目标是构建条件概率模型$P(\mathbf y|\mathbf x)$.
注:标记变量$\mathbf y$可以是结构变量,即其分量间有相关性.
	如图a中,标记变量有线性结构,图b中有树形结构
	![[Pasted image 20251226172133.png]]


令$G=<V,E>$表示结点与标记变量$\mathbf y$中元素一一对应的无向图,$y_v$ 表示与结点$v$ 对应的标记变量,$n(v)$ 表示结点$v$的近邻结点.若图$G$ 的每个变量$y_v$都满足==马尔可夫性==,即:$$ P(y_v|\mathbf x,\mathbf y_{V\backslash \{v\}})=P(y_v|\mathbf x,\mathbf y_{n(v)})$$
则$(\mathbf y,\mathbf x)$构成一个条件随机场
 ![[Pasted image 20251226172042.png]]
>[!note]
>全集中**去掉自己**得到的概率 和用所有**近邻结点**的集合的概率一样 说明与**非近邻结点无关**.

理论上来说,图$G$可具有任意结构,只要能表示标记变量之间的条件独立性关系即可.
但现实中,对其实对标记序列建模时,最常用的仍是链式结构,即 **"链式条件随机场"**(chain-structured CRF).![[Pasted image 20251226173948.png]]
与马尔可夫随机场定义联合概率的方式类似,条件概率随机场利用**势函数**和图结构上的**团**来定义条件概率$P(\mathbf y|\mathbf x)$.
给定观测序列$\mathbf x$,图14.6所示的随机场主要包含两种团,单个标记变量$\{y_i\}$和相邻的标记变量$\{y_{i-1},y_i\}$.
选择合适的势函数,即可得到形如前文中的条件概率定义
	![[概率图模型#^clique]]
	前文联合概率:
		![[概率图模型#^potentialf]]

在条件随机场中,通过选用**指数势函数**并引入**特征函数**(feature function),**条件概率**被定义为:
$$P(\mathbf y|\mathbf x) = \frac{1}{Z} \exp \left( \sum_j \sum_{i=1}^{n-1} \lambda_j t_j (y_{i+1}, y_i,\mathbf x, i) + \sum_k \sum_{i=1}^n \mu_k s_k (y_i,\mathbf x, i) \right)$$

其中
 $t_j (y_{i+1}, y_i,\mathbf x, i)$ :定义在观测序列两个相邻的标记位置上的==转移特征函数==(transition feature function),刻画相邻标记变量($y_{i+1}和 y_i$)之间的**相关关系**以及观测序列($\mathbf x$)对它们的**影响**.
 $s_k (y_i,\mathbf x, i)$ :定义在观测序列的标记位置$i$上的==状态特征函数==(status feature function),刻画观测序列对标记变量的**影响**
 注:刻画的**影响**都是 $观测\mathbf x \rightarrow 标记\mathbf y$.
 <br>
 $\lambda_j 和\mu_k$为参数
 $\boldsymbol Z$为规范化因子

## 特征函数
显然,要使用条件随机场,还需定义合适的特征函数.*通常为实值函数.以刻画数据的一些很可能成立或期望成立的经验特性.

以图14.5为例:
### 转移特征函数
若采用转移特征函数$$t_j (y_{i+1}, y_i, x, i) = 
\begin{cases}
1, & \text{if } y_{i+1} = [P], y_i = [V] \text{ and } x_i = \text{"knock"}; \\
0, & \text{otherwise},
\end{cases}$$
表示如果 $x_i$ 为"knock",那么第 i 个标记很可能为V,第 i+1个标记很可能为P.
*相邻标记变量($y_{i+1}和 y_i$)之间的**相关关系**以及观测序列($\mathbf x$)对它们的**影响**:  $x_i\rightarrow y_i \rightarrow y_{i+1}$*

### 状态特征函数
若采用状态特征函数$$s_k (y_i, x, i) = 
\begin{cases}
1, & \text{if } y_i = [V] \text{ and } x_i = \text{"knock"}; \\
0, & \text{otherwise},
\end{cases}$$
表示观测值$x_i$为"knock"时,标记很可能为V. 
*观测序列对标记变量的**影响**: $x_i\rightarrow y_i$*


对比条件随机场和马尔科夫随机场,可以看到,二者均使用团上的势函数定义概率,两者在形式上没有显著区别;
但条件随机场处理的是条件概率,马尔科夫随机场处理的是联合概率.